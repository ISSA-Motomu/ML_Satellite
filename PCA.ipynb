{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74de08e9",
   "metadata": {},
   "source": [
    "## 主成分分析\n",
    "- 教師無し学習\n",
    "> 与えられた特徴量から新たな特徴量`主成分`を作り出し、元の特徴量よりも少ない数の変数（次元）でデータを説明する手法<br>\n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "1. **中心化**\n",
    "- 各特徴量（列方向）の平均を計算し、引く\n",
    "  - axis=0<br>\n",
    "  分散で割るのは標準化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1151502",
   "metadata": {},
   "source": [
    "2. **共分散行列**　Covariance Matrix\n",
    "$$\n",
    "\\text{共分散行列 } \\Sigma = \\frac{1}{n-1} \\underbrace{(X - \\text{mean})^T}_{\\text{転置した入力}} \\cdot \\underbrace{(X - \\text{mean})}_{\\text{入力}}\n",
    "$$\n",
    "\n",
    "> 多次元であっても、共分散（二つの変数の関係）と分散を行列に並べているだけ\n",
    "\n",
    "* **$(X - \\text{mean})^T$**: $[特徴量 \\times サンプル]$ の形\n",
    "* **$(X - \\text{mean})$**: $[サンプル \\times 特徴量]$ の形\n",
    "* **掛け算の結果**: $[特徴量 \\times 特徴量]$ の正方形になります。\n",
    "<br>\n",
    "$$\n",
    "\\Sigma = \n",
    "\\begin{pmatrix}\n",
    "\\text{Var}(x) & \\text{Cov}(x, y) & \\text{Cov}(x, z) \\\\\n",
    "\\text{Cov}(y, x) & \\text{Var}(y) & \\text{Cov}(y, z) \\\\\n",
    "\\text{Cov}(z, x) & \\text{Cov}(z, y) & \\text{Var}(z)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f2f97",
   "metadata": {},
   "source": [
    "3. **固有値分解**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class PCATensorFlow:\n",
    "    \"\"\"\n",
    "    TensorFlowで主成分分析（PCA）を実装してみる\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components_ = None\n",
    "        self.mean_ = None\n",
    "        self.eigenvalues_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (tf.Tensor): Input data. Shape [n_samples, n_features]\n",
    "\n",
    "        X=tf.cast(X, tf.float32)\n",
    "            - 普通CPUは倍精度浮動小数点64bit演算で有効桁数約16桁\n",
    "            - GPUは速度を求めるために32bitの単精度演算に変換させる\n",
    "        \n",
    "        \"\"\"\n",
    "        X = tf.cast(X, tf.float32) # 単精度浮動小数点に変換\n",
    "        \n",
    "        # 1. 中心化\n",
    "        self.mean_ = tf.reduce_mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        n_samples = tf.cast(tf.shape(X)[0], tf.float32) \n",
    "\n",
    "        # 2. 共分散行列の計算\n",
    "        # NumPyの dot ではなく、TensorFlowの行列積関数を使います。\n",
    "        # ヒント: [n_features, n_samples] x [n_samples, n_features] の形にする必要があります。\n",
    "        # transpose_a=True (Xを転置) または transpose_b=True (後ろを転置) の引数を活用します。\n",
    "        cov_matrix = [Q1] / (n_samples - 1)\n",
    "\n",
    "        # 3. 固有値分解 (Self-Adjoint Eigendecomposition)\n",
    "        # 対称行列専用の高速ソルバー tf.linalg.eigh を使います。\n",
    "        # これも昇順（小さい順）で返ってきます。\n",
    "        eigenvalues, eigenvectors = tf.linalg.eigh(cov_matrix)\n",
    "\n",
    "        # 4. ソート（降順）\n",
    "        # TensorFlowにはスライシングの他に、逆順にする専用関数があります（スライスでも可）。\n",
    "        # ここではインデックス順序を反転させる処理を記述してください。\n",
    "        # 例: Pythonのスライス記法 [::-1] はTensorでも有効です。\n",
    "        sorted_indices = tf.argsort(eigenvalues, direction='DESCENDING') \n",
    "        # ↑ TFでは便利な引数がありますが、もしスライスで書くなら？\n",
    "        # 今回はシンプルに [Q2] (変数名とスライス) を答えてください。\n",
    "        # ※ tf.argsortを使わず、すでに取得した eigenvalues, eigenvectors をどう逆順にするか\n",
    "        \n",
    "        # 修正: わかりやすくするために、tf.argsortを使わずに\n",
    "        # 「得られた結果を逆順にするスライス操作」を答えてください。\n",
    "        self.eigenvalues_ = eigenvalues[[Q2]]\n",
    "        self.eigenvectors_ = eigenvectors[:, [Q2]]\n",
    "\n",
    "        # 上位n成分を取得\n",
    "        self.components_ = self.eigenvectors_[:, :self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = tf.cast(X, tf.float32)\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        # 5. 射影\n",
    "        # TensorFlowの行列積で行ってください。\n",
    "        return [Q3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b066f7",
   "metadata": {},
   "source": [
    "### データ行列 $X$\n",
    "\n",
    "Pythonでは **「行(横)＝データサンプル」**、**「列(縦)＝特徴量**とするのが不文律\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "\\text{A君の国語} & \\text{A君の数学} \\\\\n",
    "\\text{B君の国語} & \\text{B君の数学} \\\\\n",
    "\\text{C君の国語} & \\text{C君の数学}\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
