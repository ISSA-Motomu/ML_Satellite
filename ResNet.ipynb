{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b978dad",
   "metadata": {},
   "source": [
    "### ResNetの実装\n",
    "ResNetの基本式 $y = F(x) + x$\n",
    "ここで $F(x)$ は残差関数\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{ReLU}(F(x) + x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264dbbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tensorflow\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Add,BatchNormalization\n",
    "\n",
    "def residual_block(input_tensor, num_filters,strides=1):\n",
    "    \"\"\"\n",
    "    入力と出力のサイズが変わらない、基本的な残差ブロック\n",
    "    残差：Residual\n",
    "    Output = f(Input)+Input\n",
    "        ここでFは2回の畳み込みとReLU活性化\n",
    "\n",
    "    Skip Connection\n",
    "        残差分だけを学習することで、層が深くなっても勾配損失問題を防ぐ\n",
    "    \n",
    "    Conv\n",
    "        畳み込みをすると通常は画面の端っこが削れて小さくなるが、padding='same'を指定して周りに余白をつける\n",
    "        そうすると入力と出力のサイズが変わらなくなる\n",
    "\n",
    "    Add()([x,shortcut])\n",
    "        KerasのFunctional APIの書き方\n",
    "            ➀足し算マシーン(Addレイヤー)の生成、ここでAddは単純に足し算をするだけなので()の中に複雑な設定が必要ない\n",
    "            ➁足し算マシーンに[x,shortcut]というリストの形状にして足し算を実行する\n",
    "\n",
    "    Strides\n",
    "        畳み込みの移動幅を指定するパラメータ\n",
    "        1なら通常通り1pxずつ、2なら2pxずつ移動する(1個飛ばし)ので、出力サイズが半分になる\n",
    "\n",
    "    num_filters\n",
    "        各num_filterは畳み込み層で検知した特徴マップを持っている\n",
    "        例えば浅い層のフィルターではエッジ検出、深い層ではより複雑なパターンを検出する\n",
    "            エッジ：隣り合う画素(ピクセル)が急激に変化している部分\n",
    "    \"\"\"\n",
    "    shortcut = input_tensor\n",
    "\n",
    "    # 畳み込み一回目\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same',strides=strides)(input_tensor)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 畳み込み二回目\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n",
    "    x= BatchNormalization()(x)\n",
    "\n",
    "    if strides > 1 or input_tensor.shape[-1] != num_filters:\n",
    "        \"\"\"\n",
    "        サイズが変わる、またはチャンネル数が変わるとき\n",
    "        .shape[-1]は最後の要素、つまりchannnel\n",
    "        \"\"\"\n",
    "        shortcut = Conv2D(num_filters, (1, 1), padding='same',strides=strides)(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9eb64",
   "metadata": {},
   "source": [
    "### BatchNormalization()の中身について\n",
    "> ReLUにそのまま通してしまったら負数の情報が全て0になってしまうので、標準化を行う\n",
    "> $$ \\hat{x} = \\frac{x - \\mu (\\text{平均})}{\\sigma (\\text{標準偏差})} $$\n",
    "- 入ってきたデータ（ミニバッチ）に対して**標準化「平均を0,分散を1」**\n",
    "- データから平均値を引くことで中心を0としている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a130c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
    "\n",
    "def build_simple_resnet(input_shape=(128, 128, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    ResNetの簡易モデルの作成\n",
    "        画像分類タスクでは、画像の特徴の身を知りたいので畳み込み層のエンコードしか必要ない\n",
    "        セグメンテーションは位置情報も必要なのでdecodeも必要になる\n",
    "        ResNet18の18は層の数、50や152などがある\n",
    "\n",
    "    Projection(射影)\n",
    "            ➀入力の特徴マップのサイズを変換する\n",
    "            ➁フィルター数(channel数)を変更する\n",
    "\n",
    "    GlobalAveragePooling2D\n",
    "        Strides=1は画像サイズを変えないまま特徴をしっかり抽出する\n",
    "        \n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    x = residual_block(inputs, num_filters=64, strides=1)\n",
    "\n",
    "    x = residual_block(x, num_filters=64, strides=1)   #サイズを変えないまま特徴をしっかり見極める\n",
    "    x = residual_block(x, num_filters=64, strides=1) \n",
    "    \n",
    "    x = residual_block(x, num_filters=128, strides=2)   #サイズを半分にする(Projection Block)\n",
    "    x = residual_block(x, num_filters=128, strides=1) \n",
    "    \n",
    "    # --- 出口 (Output Layers) ---\n",
    "    x = GlobalAveragePooling2D()(x) \n",
    "    \n",
    "    # 最終判定 (10クラス分類)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239de01b",
   "metadata": {},
   "source": [
    "### 1. GlobalAveragePooling2D　(GAP)\n",
    "**位置情報は捨てて、特徴の強さだけを残す層**\n",
    "> 一枚の特徴マップの全画素の平均を計算して、有効な数字であればその特徴が存在すると考える\n",
    "  - 入力: $(Batch, 8, 8, 128)$ → 高さ8, 幅8, チャンネル128\n",
    "  - 出力: $(Batch, 128)$ → 長さ128のベクトル\n",
    "\n",
    "### 2. Dense （全結合層）\n",
    "  - 最後のDense層のユニット数は、**分類したいクラス数**（EuroSATなら10個）と一致させる\n",
    "  - 最後に`Softmax`関数を通すことで、出力を確率（合計すると100%）に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb41e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ダウンロードと読み込みが完了しました！\n",
      "クラス名: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "学習データ数: 21600\n",
      "検証データ数: 2700\n",
      "テストデータ数: 2700\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "\n",
    "# データを保存するディレクトリ (先ほど作った data フォルダを指定)\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "\"\"\"\n",
    "tfds.load()でdatasetをダウンロードして読みこむ\n",
    "    タプルのデータセットとinfoを返す\n",
    "        eurosatのRGBバージョンを使用する\n",
    "        as_supervised=True で(画像, ラベル)のタプルで返す\n",
    "        split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'] で学習・検証・テストに分割\n",
    "\n",
    "\"\"\"\n",
    "(train_ds, val_ds, test_ds), info = tfds.load(\n",
    "    'eurosat/rgb',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    data_dir=DATA_DIR,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "print(\"ダウンロードと読み込みが完了しました！\")\n",
    "print(f\"クラス名: {info.features['label'].names}\")\n",
    "print(f\"学習データ数: {len(train_ds)}\")\n",
    "print(f\"検証データ数: {len(val_ds)}\")\n",
    "print(f\"テストデータ数: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73845dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(f\"現在のポリシー: {policy.compute_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106012a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: (32, 64, 64, 3)\n",
      "Label Batch Shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Setting constants\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def preprocess_data(image, label):\n",
    "    \"\"\"\n",
    "    画像データの前処理を行う関数\n",
    "        1. サイズを確実に合わせる (リサイズ)\n",
    "            - `tf.image.resize()`はTensorflowのimageモジュールの中にある関数\n",
    "            - interpolation 補間：いい感じに新しい色を計算して埋める\n",
    "            > 有限要素法のバイリニア補間\n",
    "                すべての画素に対して、周囲の整数格子点の値を使って線形補間を行う方法\n",
    "\n",
    "        2. 正規化 (Normalization)\n",
    "            - 画像のRGB各チャンネルの値を0から1の範囲にスケーリングする\n",
    "                - 正規化しないと計算量が膨大になる\n",
    "        \n",
    "        .take(n)\n",
    "            - データセットから最初のn個の要素を取得するメソッド\n",
    "    \"\"\"\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)) # 1.サイズを確実に合わせる(リサイズ)\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # 2.正規化(Normalization)\n",
    "    return image, label\n",
    "\n",
    "# --- パイプラインの構築 ---\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_batches = train_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE) \\\n",
    "                        .shuffle(buffer_size=1000) \\\n",
    "                        .batch(BATCH_SIZE) \\\n",
    "                        .prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_batches = val_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE) \\\n",
    "                    .batch(BATCH_SIZE) \\\n",
    "                    .prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 確認\n",
    "for img_batch, label_batch in train_batches.take(1):\n",
    "    print(f\"Image Batch Shape: {img_batch.shape}\")\n",
    "    print(f\"Label Batch Shape: {label_batch.shape}\")\n",
    "\n",
    "    # Image: (32, 64, 64, 3) -> 32枚, 64x64ピクセル, 3チャンネル(RGB)\n",
    "    # Label: (32,) -> 32個の正解ラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf5151",
   "metadata": {},
   "source": [
    "### データパイプライン DataPipeLine\n",
    "> データパイプラインとは、データをストレージから読み出し、計算モデルが処理可能なテンソルへと変換し、GPUやTPUへ供給する一連の処理工程、およびそれを実装したソフトウェアアーキテクチャを指す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5144ac23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_simple_resnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# input_shapeは (64, 64, 3), クラス数は 10\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mbuild_simple_resnet\u001b[49m(input_shape=(\u001b[32m64\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m3\u001b[39m), num_classes=\u001b[32m10\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- コンパイル (学習ルールの設定) ---\u001b[39;00m\n\u001b[32m      5\u001b[39m model.compile(\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# AIの「間違いの修正方法」を指定します。Adamは最も一般的で優秀な修正担当者です。\u001b[39;00m\n\u001b[32m      7\u001b[39m     optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m,    \u001b[38;5;66;03m# Optimizer (最適化アルゴリズム): 'adam'\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'build_simple_resnet' is not defined"
     ]
    }
   ],
   "source": [
    "# input_shapeは (64, 64, 3), クラス数は 10\n",
    "model = build_simple_resnet(input_shape=(64, 64, 3), num_classes=10)\n",
    "\n",
    "# --- コンパイル (学習ルールの設定) ---\n",
    "model.compile(\n",
    "    # AIの「間違いの修正方法」を指定します。Adamは最も一般的で優秀な修正担当者です。\n",
    "    optimizer='adam',    # Optimizer (最適化アルゴリズム): 'adam'\n",
    "\n",
    "    # Loss (損失関数): 'SparseCategoricalCrossentropy'\n",
    "    # 正解が整数(0, 1, 2...)の場合は 'Sparse...' を使います。\n",
    "    # from_logits=True は、AIの生の出力を確率に変換してから計算しろ、という指示です。\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\n",
    "    # Metrics (評価指標): 'accuracy'\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# モデルの設計図確認\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9678ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:717: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1064s\u001b[0m 2s/step - accuracy: 0.6689 - loss: 0.9373 - val_accuracy: 0.3926 - val_loss: 4.6740\n",
      "Epoch 2/5\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1082s\u001b[0m 2s/step - accuracy: 0.7765 - loss: 0.6405 - val_accuracy: 0.5067 - val_loss: 1.7450\n",
      "Epoch 3/5\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1096s\u001b[0m 2s/step - accuracy: 0.8378 - loss: 0.4796 - val_accuracy: 0.6370 - val_loss: 1.9985\n",
      "Epoch 4/5\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1072s\u001b[0m 2s/step - accuracy: 0.8656 - loss: 0.3897 - val_accuracy: 0.7067 - val_loss: 0.9175\n",
      "Epoch 5/5\n",
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1102s\u001b[0m 2s/step - accuracy: 0.8864 - loss: 0.3371 - val_accuracy: 0.7581 - val_loss: 0.7406\n"
     ]
    }
   ],
   "source": [
    "# --- 学習の実行 ---\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    validation_data=val_batches,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1690ad5",
   "metadata": {},
   "source": [
    "###　Result\n",
    "- 自分のPCのCPU,Core i7だと90分かかった![Result of CPU Learning](images/122101.png)\n",
    "<br>\n",
    "- AI工房のGPUサーバーを使うと40秒程度で5epoch終了->**135倍!!のスピード**![Result of GPU Learning](images/122102.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
