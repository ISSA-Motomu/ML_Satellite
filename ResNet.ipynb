{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b978dad",
   "metadata": {},
   "source": [
    "### ResNetの実装\n",
    "ResNetの基本式 $y = F(x) + x$\n",
    "ここで $F(x)$ は残差関数\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{ReLU}(F(x) + x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9eb64",
   "metadata": {},
   "source": [
    "### BatchNormalization()の中身について\n",
    "> ReLUにそのまま通してしまったら負数の情報が全て0になってしまうので、標準化を行う\n",
    "> $$ \\hat{x} = \\frac{x - \\mu (\\text{平均})}{\\sigma (\\text{標準偏差})} $$\n",
    "- 入ってきたデータ（ミニバッチ）に対して**標準化「平均を0,分散を1」**\n",
    "- データから平均値を引くことで中心を0としている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ea8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tensorflow\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Add, BatchNormalization\n",
    "\n",
    "\n",
    "def residual_block(input_tensor, num_filters, strides=1):\n",
    "    \"\"\"\n",
    "    入力と出力のサイズが変わらない、基本的な残差ブロック\n",
    "    残差：Residual\n",
    "    Output = f(Input)+Input\n",
    "        ここでFは2回の畳み込みとReLU活性化\n",
    "\n",
    "    Skip Connection\n",
    "        残差分だけを学習することで、層が深くなっても勾配損失問題を防ぐ\n",
    "\n",
    "    Conv\n",
    "        畳み込みをすると通常は画面の端っこが削れて小さくなるが、padding='same'を指定して周りに余白をつける\n",
    "        そうすると入力と出力のサイズが変わらなくなる\n",
    "\n",
    "    Add()([x,shortcut])\n",
    "        KerasのFunctional APIの書き方\n",
    "            ➀足し算マシーン(Addレイヤー)の生成、ここでAddは単純に足し算をするだけなので()の中に複雑な設定が必要ない\n",
    "            ➁足し算マシーンに[x,shortcut]というリストの形状にして足し算を実行する\n",
    "\n",
    "    Strides\n",
    "        畳み込みの移動幅を指定するパラメータ\n",
    "        1なら通常通り1pxずつ、2なら2pxずつ移動する(1個飛ばし)ので、出力サイズが半分になる\n",
    "\n",
    "    num_filters\n",
    "        各num_filterは畳み込み層で検知した特徴マップを持っている\n",
    "        例えば浅い層のフィルターではエッジ検出、深い層ではより複雑なパターンを検出する\n",
    "            エッジ：隣り合う画素(ピクセル)が急激に変化している部分\n",
    "    \"\"\"\n",
    "    shortcut = input_tensor\n",
    "\n",
    "    # 畳み込み一回目\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\", strides=strides)(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # 畳み込み二回目\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if strides > 1 or input_tensor.shape[-1] != num_filters:\n",
    "        \"\"\"\n",
    "        サイズが変わる、またはチャンネル数が変わるとき\n",
    "        .shape[-1]は最後の要素、つまりchannnel\n",
    "        \"\"\"\n",
    "        shortcut = Conv2D(num_filters, (1, 1), padding=\"same\", strides=strides)(\n",
    "            shortcut\n",
    "        )\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
    "\n",
    "\n",
    "def build_simple_resnet(input_shape=(128, 128, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    ResNetの簡易モデルの作成\n",
    "        画像分類タスクでは、画像の特徴の身を知りたいので畳み込み層のエンコードしか必要ない\n",
    "        セグメンテーションは位置情報も必要なのでdecodeも必要になる\n",
    "        ResNet18の18は層の数、50や152などがある\n",
    "\n",
    "    Projection(射影)\n",
    "            ➀入力の特徴マップのサイズを変換する\n",
    "            ➁フィルター数(channel数)を変更する\n",
    "\n",
    "    GlobalAveragePooling2D\n",
    "        Strides=1は画像サイズを変えないまま特徴をしっかり抽出する\n",
    "\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    x = residual_block(inputs, num_filters=64, strides=1)\n",
    "\n",
    "    x = residual_block(\n",
    "        x, num_filters=64, strides=1\n",
    "    )  # サイズを変えないまま特徴をしっかり見極める\n",
    "    x = residual_block(x, num_filters=64, strides=1)\n",
    "\n",
    "    x = residual_block(\n",
    "        x, num_filters=128, strides=2\n",
    "    )  # サイズを半分にする(Projection Block)\n",
    "    x = residual_block(x, num_filters=128, strides=1)\n",
    "\n",
    "    # --- 出口 (Output Layers) ---\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # 最終判定 (10クラス分類)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239de01b",
   "metadata": {},
   "source": [
    "### 1. GlobalAveragePooling2D　(GAP)\n",
    "**位置情報は捨てて、特徴の強さだけを残す層**\n",
    "> 一枚の特徴マップの全画素の平均を計算して、有効な数字であればその特徴が存在すると考える\n",
    "  - 入力: $(Batch, 8, 8, 128)$ → 高さ8, 幅8, チャンネル128\n",
    "  - 出力: $(Batch, 128)$ → 長さ128のベクトル\n",
    "\n",
    "### 2. Dense （全結合層）\n",
    "  - 最後のDense層のユニット数は、**分類したいクラス数**（EuroSATなら10個）と一致させる\n",
    "  - 最後に`Softmax`関数を通すことで、出力を確率（合計すると100%）に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb41e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ダウンロードと読み込みが完了しました！\n",
      "クラス名: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "学習データ数: 21600\n",
      "検証データ数: 2700\n",
      "テストデータ数: 2700\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "\n",
    "# データを保存するディレクトリ (先ほど作った data フォルダを指定)\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "\"\"\"\n",
    "tfds.load()でdatasetをダウンロードして読みこむ\n",
    "    タプルのデータセットとinfoを返す\n",
    "        eurosatのRGBバージョンを使用する\n",
    "        as_supervised=True で(画像, ラベル)のタプルで返す\n",
    "        split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'] で学習・検証・テストに分割\n",
    "\n",
    "\"\"\"\n",
    "(train_ds, val_ds, test_ds), info = tfds.load(\n",
    "    \"eurosat/rgb\",\n",
    "    split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "    data_dir=DATA_DIR,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "print(\"ダウンロードと読み込みが完了しました！\")\n",
    "print(f\"クラス名: {info.features['label'].names}\")\n",
    "print(f\"学習データ数: {len(train_ds)}\")\n",
    "print(f\"検証データ数: {len(val_ds)}\")\n",
    "print(f\"テストデータ数: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73845dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在のポリシー: float16\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(f\"現在のポリシー: {policy.compute_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106012a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: (32, 64, 64, 3)\n",
      "Label Batch Shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Setting constants\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 256  # AI工房のRTX 3090 + VRAM 24GB\n",
    "\n",
    "\n",
    "def preprocess_data(image, label):\n",
    "    \"\"\"\n",
    "    画像データの前処理を行う関数\n",
    "        1. サイズを確実に合わせる (リサイズ)\n",
    "            - `tf.image.resize()`はTensorflowのimageモジュールの中にある関数\n",
    "            - interpolation 補間：いい感じに新しい色を計算して埋める\n",
    "            > 有限要素法のバイリニア補間\n",
    "                すべての画素に対して、周囲の整数格子点の値を使って線形補間を行う方法\n",
    "\n",
    "        2. 正規化 (Normalization)\n",
    "            - 画像のRGB各チャンネルの値を0から1の範囲にスケーリングする\n",
    "                - 正規化しないと計算量が膨大になる\n",
    "\n",
    "        .take(n)\n",
    "            - データセットから最初のn個の要素を取得するメソッド\n",
    "    \"\"\"\n",
    "    image = tf.image.resize(\n",
    "        image, (IMG_SIZE, IMG_SIZE)\n",
    "    )  # 1.サイズを確実に合わせる(リサイズ)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # 2.正規化(Normalization)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# --- パイプラインの構築 ---\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_batches = (\n",
    "    train_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .shuffle(buffer_size=1000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_batches = (\n",
    "    val_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 確認\n",
    "for img_batch, label_batch in train_batches.take(1):\n",
    "    print(f\"Image Batch Shape: {img_batch.shape}\")\n",
    "    print(f\"Label Batch Shape: {label_batch.shape}\")\n",
    "\n",
    "    # Image: (32, 64, 64, 3) -> 32枚, 64x64ピクセル, 3チャンネル(RGB)\n",
    "    # Label: (32,) -> 32個の正解ラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf5151",
   "metadata": {},
   "source": [
    "### データパイプライン DataPipeLine\n",
    "> データパイプラインとは、データをストレージから読み出し、計算モデルが処理可能なテンソルへと変換し、GPUやTPUへ供給する一連の処理工程、およびそれを実装したソフトウェアアーキテクチャを指す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shapeは (64, 64, 3), クラス数は 10\n",
    "model = build_simple_resnet(input_shape=(64, 64, 3), num_classes=10)\n",
    "\n",
    "# --- コンパイル (学習ルールの設定) ---\n",
    "model.compile(\n",
    "    # AIの「間違いの修正方法」を指定します。Adamは最も一般的で優秀な修正担当者です。\n",
    "    optimizer=\"adam\",  # Optimizer (最適化アルゴリズム): 'adam'\n",
    "    # Loss (損失関数): 'SparseCategoricalCrossentropy'\n",
    "    # from_logits=True は、AIの生の出力を確率に変換してから計算しろ、という指示です。\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    # Metrics:'accuracy'\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# モデルの設計図確認\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9678ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 学習の実行 ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:919\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    913\u001b[39m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[32m    914\u001b[39m   filtered_flat_args = (\n\u001b[32m    915\u001b[39m       \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.unpack_inputs(\n\u001b[32m    916\u001b[39m           bound_args\n\u001b[32m    917\u001b[39m       )\n\u001b[32m    918\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    920\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[32m    925\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 学習の実行 ---\n",
    "history = model.fit(train_batches, validation_data=val_batches, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1690ad5",
   "metadata": {},
   "source": [
    "###　Result\n",
    "- 自分のPCのCPU,Core i7だと90分かかった![Result of CPU Learning](images/122101.png)\n",
    "<br>\n",
    "- AI工房のGPUサーバーを使うと40秒程度で5epoch終了->**135倍!!のスピード**![Result of GPU Learning](images/122102.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- テストデータの準備と評価 ---\n",
    "\n",
    "# テストデータにも同様の前処理（リサイズ・正規化）とバッチ化を適用\n",
    "test_batches = (\n",
    "    test_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "# モデルの評価 (未学習データでの性能確認)\n",
    "print(\"テストデータで評価を実行します...\")\n",
    "test_loss, test_acc = model.evaluate(test_batches)\n",
    "\n",
    "print(f\"\\nテストデータの正解率: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# クラス名を取得\n",
    "class_names = info.features[\"label\"].names\n",
    "\n",
    "\n",
    "def plot_predictions(dataset, model, num_images=9):\n",
    "    \"\"\"\n",
    "    テストデータに対して推論を行い、画像と予測結果を表示する関数\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    # データセットから1バッチ(32枚)だけ取り出す\n",
    "    for images, labels in dataset.take(1):\n",
    "        # 推論の実行 (確率が出力される)\n",
    "        predictions = model.predict(images)\n",
    "        # 最も確率が高いクラスのインデックスを取得\n",
    "        pred_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # 指定枚数分だけ表示\n",
    "        for i in range(min(num_images, len(images))):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i])\n",
    "\n",
    "            # ラベル名の取得\n",
    "            true_label = class_names[labels[i]]\n",
    "            pred_label = class_names[pred_indices[i]]\n",
    "            confidence = 100 * np.max(predictions[i])\n",
    "\n",
    "            # 正解なら青、不正解なら赤でタイトルを表示\n",
    "            color = \"blue\" if labels[i] == pred_indices[i] else \"red\"\n",
    "\n",
    "            plt.title(\n",
    "                f\"Pred: {pred_label} ({confidence:.1f}%)\\nTrue: {true_label}\",\n",
    "                color=color,\n",
    "            )\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 可視化の実行\n",
    "print(\"推論結果の可視化:\")\n",
    "plot_predictions(test_batches, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba36e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# --- 分析用データの準備 ---\n",
    "print(\"詳細なエラー分析を実行します...\")\n",
    "\n",
    "# 全テストデータの予測と正解ラベルの取得\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "# データセットから全データを取得\n",
    "for img_batch, label_batch in test_batches:\n",
    "    all_images.append(img_batch.numpy())\n",
    "    all_labels.append(label_batch.numpy())\n",
    "\n",
    "x_test = np.concatenate(all_images)\n",
    "y_test = np.concatenate(all_labels)\n",
    "\n",
    "# 推論実行 (バッチ処理しているので高速)\n",
    "predictions = model.predict(x_test, verbose=0)\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "max_probs = np.max(predictions, axis=1)\n",
    "\n",
    "# --- 1. 混同行列 (Confusion Matrix) ---\n",
    "conf_matrix = confusion_matrix(y_test, pred_labels)\n",
    "\n",
    "# --- 2. 「自信満々に間違えた」データの抽出 ---\n",
    "incorrect_indices = np.where(pred_labels != y_test)[0]\n",
    "# 予測確率(自信)が高い順にソートしてトップを取得\n",
    "sorted_incorrect_indices = incorrect_indices[\n",
    "    np.argsort(max_probs[incorrect_indices])[::-1]\n",
    "]\n",
    "\n",
    "\n",
    "# --- 3. Grad-CAM (判断根拠の可視化) ---\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    \"\"\"\n",
    "    Grad-CAM ヒートマップを生成する関数\n",
    "\n",
    "    \"\"\"\n",
    "    if len(img_array.shape) == 3:\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "\n",
    "def find_target_layer(model):\n",
    "    for layer in reversed(model.layers):\n",
    "        # layer.output_shape は古いKerasの仕様です。代わりに layer.output.shape を使用します。\n",
    "        try:\n",
    "            if hasattr(layer, \"output\") and len(layer.output.shape) == 4:\n",
    "                return layer.name\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "target_layer_name = find_target_layer(model)\n",
    "\n",
    "# --- 可視化プロット ---\n",
    "plt.figure(figsize=(20, 10))\n",
    "gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 0.5, 0.5])\n",
    "\n",
    "# 左側: 混同行列\n",
    "ax_cm = plt.subplot(gs[:, :2])\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    ax=ax_cm,\n",
    ")\n",
    "ax_cm.set_title(\"Confusion Matrix\", fontsize=16)\n",
    "ax_cm.set_ylabel(\"True Label\", fontsize=14)\n",
    "ax_cm.set_xlabel(\"Predicted Label\", fontsize=14)\n",
    "\n",
    "# 右側: 自信満々に間違えた画像 Top 4\n",
    "num_display = 4\n",
    "for i in range(min(num_display, len(sorted_incorrect_indices))):\n",
    "    idx = sorted_incorrect_indices[i]\n",
    "    img = x_test[idx]\n",
    "    true_lb = class_names[y_test[idx]]\n",
    "    pred_lb = class_names[pred_labels[idx]]\n",
    "    conf = max_probs[idx]\n",
    "\n",
    "    # Grad-CAM ヒートマップ\n",
    "    heatmap = make_gradcam_heatmap(\n",
    "        img, model, target_layer_name, pred_index=pred_labels[idx]\n",
    "    )\n",
    "\n",
    "    # リサイズと重ね合わせ表示\n",
    "    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], (IMG_SIZE, IMG_SIZE))\n",
    "    heatmap_resized = tf.squeeze(heatmap_resized).numpy()\n",
    "\n",
    "    # グリッド配置 (右半分の領域を使う)\n",
    "    row = i // 2\n",
    "    col = 2 + (i % 2)\n",
    "    ax_img = plt.subplot(gs[row, col])\n",
    "\n",
    "    ax_img.imshow(img)\n",
    "    ax_img.imshow(heatmap_resized, alpha=0.5, cmap=\"jet\")\n",
    "\n",
    "    # 枠線の色 (赤: 間違い)\n",
    "    for spine in ax_img.spines.values():\n",
    "        spine.set_edgecolor(\"red\")\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    ax_img.set_title(\n",
    "        f\"True: {true_lb}\\nPred: {pred_lb}\\nConf: {conf:.1%}\",\n",
    "        fontsize=12,\n",
    "        color=\"red\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax_img.set_xticks([])\n",
    "    ax_img.set_yticks([])\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Error Analysis & Grad-CAM (Target Layer: {target_layer_name})\", fontsize=20\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 考察コメントの例示\n",
    "print(\"\\n=== 考察コメントの例 ===\")\n",
    "print(\n",
    "    \"Grad-CAMのヒートマップ（赤色部分）を確認することで、モデルが画像のどこを見て判断したかが分かります。\"\n",
    ")\n",
    "print(\n",
    "    \"これを用いて、『形ではなく色だけで判断してしまっている』などの誤答原因を分析できます。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_color_histograms(dataset, class_names, target_classes):\n",
    "    \"\"\"\n",
    "    指定した2つのクラスの「色の分布」を比較する\n",
    "    target_classes: ['SeaLake', 'Pasture'] のように指定\n",
    "    \"\"\"\n",
    "    colors = [\"Red\", \"Green\", \"Blue\"]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # データを少し集める\n",
    "    target_images = {name: [] for name in target_classes}\n",
    "\n",
    "    for img, label in dataset.take(200):  # 200枚くらいチェック\n",
    "        name = class_names[label.numpy()]\n",
    "        if name in target_classes:\n",
    "            target_images[name].append(img.numpy())\n",
    "\n",
    "    # 各チャンネルごとにヒストグラムを描画\n",
    "    for i, color in enumerate(colors):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        for name in target_classes:\n",
    "            if len(target_images[name]) == 0:\n",
    "                continue\n",
    "\n",
    "            # 全画像のピクセル値をまとめる\n",
    "            pixels = np.concatenate(target_images[name])\n",
    "            pixel_values = pixels[:, :, i].flatten()  # 1次元に伸ばす\n",
    "\n",
    "            plt.hist(pixel_values, bins=50, alpha=0.5, label=name, density=True)\n",
    "\n",
    "        plt.title(f\"{color} Channel Distribution\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Pixel Value (0-1)\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 実行\n",
    "plot_color_histograms(test_ds, info.features[\"label\"].names, [\"SeaLake\", \"Pasture\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f21044a",
   "metadata": {},
   "source": [
    "### RGB 3D分布による誤分類原因の分析\n",
    "\n",
    "ResNetの誤分類がRGB（色）に起因するかを確認するため、以下の分析を行います。\n",
    "\n",
    "#### 分析手法\n",
    "1. **3D散布図**: 各画像のRGB平均値をプロット\n",
    "2. **定量分析**: 誤分類サンプルが「元クラス」と「混同先クラス」どちらに近いか計算\n",
    "3. **統計的検定**: 正解/誤分類サンプル間でRGB特性に有意差があるか検証\n",
    "4. **RGB分散分析**: 画像内の色のばらつきが誤分類に関係するか調査\n",
    "\n",
    "#### 考察の観点\n",
    "\n",
    "| 観察結果 | 解釈 |\n",
    "|---------|------|\n",
    "| 誤分類サンプルが混同先クラスの領域にある | → RGBの類似性が原因 |\n",
    "| 誤分類サンプルが元クラス内に散在している | → RGB以外の要因（テクスチャ、エッジ、形状）が原因 |\n",
    "| 誤分類サンプルに特定のRGB傾向がない | → RGBは分類に寄与していない可能性 |\n",
    "\n",
    "#### RGB平均の限界\n",
    "- 画像全体の平均値は**局所的な特徴を消してしまう**\n",
    "- 同じRGB平均でも、テクスチャや空間パターンは異なりうる\n",
    "- 衛星画像では、物体の**配置**や**繰り返しパターン**が重要なことが多い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def plot_rgb_3d_interactive(x_test, y_test, pred_labels, class_names):\n",
    "    \"\"\"\n",
    "    RGB平均値を3D空間にプロットし、正解/誤分類を可視化する関数\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_test : 正規化済みテスト画像 (0-1)\n",
    "    y_test : 正解ラベル\n",
    "    pred_labels : 予測ラベル\n",
    "    class_names : クラス名リスト\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. 各画像のRGB平均値を計算\n",
    "    # ==========================================\n",
    "    print(\"各画像のRGB平均値を計算中...\")\n",
    "\n",
    "    # 画像ごとのRGB平均 (0-255スケールに戻す)\n",
    "    rgb_means = np.mean(x_test, axis=(1, 2)) * 255  # shape: (N, 3)\n",
    "\n",
    "    # 正解/誤分類のマスク\n",
    "    correct_mask = pred_labels == y_test\n",
    "    incorrect_mask = ~correct_mask\n",
    "\n",
    "    print(f\"正解数: {np.sum(correct_mask)}, 誤分類数: {np.sum(incorrect_mask)}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. Plotlyでインタラクティブ3Dプロット\n",
    "    # ==========================================\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # カラーマップ (10クラス用)\n",
    "    colors = [\n",
    "        \"#1f77b4\",\n",
    "        \"#ff7f0e\",\n",
    "        \"#2ca02c\",\n",
    "        \"#d62728\",\n",
    "        \"#9467bd\",\n",
    "        \"#8c564b\",\n",
    "        \"#e377c2\",\n",
    "        \"#7f7f7f\",\n",
    "        \"#bcbd22\",\n",
    "        \"#17becf\",\n",
    "    ]\n",
    "\n",
    "    # --- 正解サンプル (薄く表示) ---\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = (y_test == i) & correct_mask\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "\n",
    "        data = rgb_means[mask]\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=data[:, 0],\n",
    "                y=data[:, 1],\n",
    "                z=data[:, 2],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=3, color=colors[i], opacity=0.3),\n",
    "                name=f\"{class_name} (正解)\",\n",
    "                legendgroup=class_name,\n",
    "                hovertemplate=f\"<b>{class_name}</b><br>R: %{{x:.1f}}<br>G: %{{y:.1f}}<br>B: %{{z:.1f}}<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # --- 誤分類サンプル (大きく目立たせる) ---\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # このクラスに属する画像で誤分類されたもの\n",
    "        mask = (y_test == i) & incorrect_mask\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "\n",
    "        data = rgb_means[mask]\n",
    "        pred_class_indices = pred_labels[mask]\n",
    "\n",
    "        # ホバーテキストに予測クラスも表示\n",
    "        hover_texts = [\n",
    "            f\"<b>True: {class_name}</b><br>Pred: {class_names[p]}<br>R: {r:.1f}, G: {g:.1f}, B: {b:.1f}\"\n",
    "            for (r, g, b), p in zip(data, pred_class_indices)\n",
    "        ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=data[:, 0],\n",
    "                y=data[:, 1],\n",
    "                z=data[:, 2],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=colors[i],\n",
    "                    opacity=1.0,\n",
    "                    symbol=\"x\",  # ×マーカーで誤分類を強調\n",
    "                    line=dict(width=2, color=\"black\"),\n",
    "                ),\n",
    "                name=f\"{class_name} (誤分類)\",\n",
    "                legendgroup=class_name,\n",
    "                hovertemplate=\"%{text}<extra></extra>\",\n",
    "                text=hover_texts,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # --- 各クラスの重心を表示 ---\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = y_test == i\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "\n",
    "        center = np.mean(rgb_means[mask], axis=0)\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[center[0]],\n",
    "                y=[center[1]],\n",
    "                z=[center[2]],\n",
    "                mode=\"markers+text\",\n",
    "                marker=dict(\n",
    "                    size=15,\n",
    "                    color=colors[i],\n",
    "                    symbol=\"diamond\",\n",
    "                    line=dict(width=3, color=\"white\"),\n",
    "                ),\n",
    "                text=[class_name],\n",
    "                textposition=\"top center\",\n",
    "                textfont=dict(size=12, color=\"black\"),\n",
    "                name=f\"{class_name} 重心\",\n",
    "                legendgroup=class_name,\n",
    "                showlegend=False,\n",
    "                hovertemplate=f\"<b>{class_name} 重心</b><br>R: {center[0]:.1f}<br>G: {center[1]:.1f}<br>B: {center[2]:.1f}<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # レイアウト設定\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"RGB Distribution - 正解 vs 誤分類<br><sub>×マーカー = 誤分類サンプル</sub>\",\n",
    "            font=dict(size=20),\n",
    "        ),\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Red Mean\",\n",
    "            yaxis_title=\"Green Mean\",\n",
    "            zaxis_title=\"Blue Mean\",\n",
    "            xaxis=dict(range=[0, 255]),\n",
    "            yaxis=dict(range=[0, 255]),\n",
    "            zaxis=dict(range=[0, 255]),\n",
    "            aspectmode=\"cube\",\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=1.02, font=dict(size=10)),\n",
    "        margin=dict(r=200),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. 誤分類の統計情報\n",
    "    # ==========================================\n",
    "    print(\"\\n=== 誤分類のRGB分析 ===\")\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        correct = rgb_means[(y_test == i) & correct_mask]\n",
    "        incorrect = rgb_means[(y_test == i) & incorrect_mask]\n",
    "\n",
    "        if len(incorrect) > 0 and len(correct) > 0:\n",
    "            correct_mean = np.mean(correct, axis=0)\n",
    "            incorrect_mean = np.mean(incorrect, axis=0)\n",
    "            diff = incorrect_mean - correct_mean\n",
    "\n",
    "            print(f\"\\n【{class_name}】 誤分類: {len(incorrect)}件\")\n",
    "            print(\n",
    "                f\"  正解サンプルの平均RGB: R={correct_mean[0]:.1f}, G={correct_mean[1]:.1f}, B={correct_mean[2]:.1f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  誤分類サンプルの平均RGB: R={incorrect_mean[0]:.1f}, G={incorrect_mean[1]:.1f}, B={incorrect_mean[2]:.1f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  差分 (誤分類-正解): ΔR={diff[0]:+.1f}, ΔG={diff[1]:+.1f}, ΔB={diff[2]:+.1f}\"\n",
    "            )\n",
    "\n",
    "    return rgb_means, correct_mask, incorrect_mask\n",
    "\n",
    "\n",
    "# 実行 (前のセルで x_test, y_test, pred_labels が定義済みの前提)\n",
    "print(\"3D RGB分布をプロットします...\")\n",
    "rgb_means, correct_mask, incorrect_mask = plot_rgb_3d_interactive(\n",
    "    x_test, y_test, pred_labels, class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d04483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_pairs_rgb(x_test, y_test, pred_labels, class_names, top_n=5):\n",
    "    \"\"\"\n",
    "    誤分類が多いクラスペアのRGB分布を詳細比較する\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    from scipy import stats\n",
    "\n",
    "    # 誤分類ペアをカウント\n",
    "    incorrect_mask = pred_labels != y_test\n",
    "    pairs = [(y_test[i], pred_labels[i]) for i in np.where(incorrect_mask)[0]]\n",
    "    pair_counts = Counter(pairs)\n",
    "\n",
    "    print(\"=== 誤分類ペア Top {} ===\".format(top_n))\n",
    "    for (true_idx, pred_idx), count in pair_counts.most_common(top_n):\n",
    "        print(f\"  {class_names[true_idx]} → {class_names[pred_idx]}: {count}件\")\n",
    "\n",
    "    if len(pair_counts) == 0:\n",
    "        print(\"誤分類がありません！\")\n",
    "        return\n",
    "\n",
    "    top_pairs = pair_counts.most_common(min(top_n, 4))\n",
    "\n",
    "    # RGB平均値\n",
    "    rgb_means = np.mean(x_test, axis=(1, 2)) * 255\n",
    "\n",
    "    # サブプロット作成 (2x2)\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[\n",
    "            [{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}],\n",
    "            [{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}],\n",
    "        ],\n",
    "        subplot_titles=[\n",
    "            f\"{class_names[t]} ↔ {class_names[p]} ({c}件)\" for (t, p), c in top_pairs\n",
    "        ]\n",
    "        + [\"\"] * (4 - len(top_pairs)),\n",
    "    )\n",
    "\n",
    "    colors_correct = [\"#2196F3\", \"#4CAF50\"]\n",
    "    colors_incorrect = [\"#F44336\", \"#FF9800\"]\n",
    "\n",
    "    # ==========================================\n",
    "    # 定量分析の準備\n",
    "    # ==========================================\n",
    "    analysis_results = []\n",
    "\n",
    "    for idx, ((true_idx, pred_idx), count) in enumerate(top_pairs):\n",
    "        row = idx // 2 + 1\n",
    "        col = idx % 2 + 1\n",
    "\n",
    "        true_name = class_names[true_idx]\n",
    "        pred_name = class_names[pred_idx]\n",
    "\n",
    "        true_class_mask = y_test == true_idx\n",
    "        true_correct = true_class_mask & (pred_labels == y_test)\n",
    "        true_wrong = true_class_mask & (pred_labels == pred_idx)\n",
    "\n",
    "        pred_class_mask = y_test == pred_idx\n",
    "        pred_correct = pred_class_mask & (pred_labels == y_test)\n",
    "\n",
    "        # --- 定量分析 ---\n",
    "        correct_data = rgb_means[true_correct]\n",
    "        wrong_data = rgb_means[true_wrong]\n",
    "        pred_class_data = rgb_means[pred_correct]\n",
    "\n",
    "        if len(wrong_data) > 0 and len(correct_data) > 0 and len(pred_class_data) > 0:\n",
    "            # 各クラスの重心\n",
    "            correct_center = np.mean(correct_data, axis=0)\n",
    "            wrong_center = np.mean(wrong_data, axis=0)\n",
    "            pred_center = np.mean(pred_class_data, axis=0)\n",
    "\n",
    "            # 誤分類サンプルが「元クラス重心」と「混同先クラス重心」のどちらに近いか\n",
    "            dist_to_own = np.linalg.norm(wrong_center - correct_center)\n",
    "            dist_to_pred = np.linalg.norm(wrong_center - pred_center)\n",
    "\n",
    "            # 誤分類サンプルの分散（散らばり具合）\n",
    "            wrong_variance = np.mean(np.var(wrong_data, axis=0))\n",
    "            correct_variance = np.mean(np.var(correct_data, axis=0))\n",
    "\n",
    "            # クラス間の重複度（重心間距離 / 分散）\n",
    "            class_separation = np.linalg.norm(correct_center - pred_center)\n",
    "\n",
    "            analysis_results.append(\n",
    "                {\n",
    "                    \"pair\": f\"{true_name} → {pred_name}\",\n",
    "                    \"count\": count,\n",
    "                    \"dist_to_own\": dist_to_own,\n",
    "                    \"dist_to_pred\": dist_to_pred,\n",
    "                    \"closer_to\": \"pred_class\"\n",
    "                    if dist_to_pred < dist_to_own\n",
    "                    else \"own_class\",\n",
    "                    \"class_separation\": class_separation,\n",
    "                    \"wrong_variance\": wrong_variance,\n",
    "                    \"correct_variance\": correct_variance,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # プロット\n",
    "        if len(correct_data) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=correct_data[:, 0],\n",
    "                    y=correct_data[:, 1],\n",
    "                    z=correct_data[:, 2],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=4, color=colors_correct[0], opacity=0.5),\n",
    "                    name=f\"{true_name} (正解)\",\n",
    "                    showlegend=(idx == 0),\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "        if len(wrong_data) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=wrong_data[:, 0],\n",
    "                    y=wrong_data[:, 1],\n",
    "                    z=wrong_data[:, 2],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        size=8, color=colors_incorrect[0], opacity=1.0, symbol=\"x\"\n",
    "                    ),\n",
    "                    name=f\"{true_name}→{pred_name} (誤分類)\",\n",
    "                    showlegend=(idx == 0),\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "        if len(pred_class_data) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=pred_class_data[:, 0],\n",
    "                    y=pred_class_data[:, 1],\n",
    "                    z=pred_class_data[:, 2],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=4, color=colors_correct[1], opacity=0.5),\n",
    "                    name=f\"{pred_name} (正解)\",\n",
    "                    showlegend=(idx == 0),\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"誤分類ペア別 RGB分布比較\",\n",
    "        height=900,\n",
    "        width=1100,\n",
    "    )\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        fig.update_scenes(\n",
    "            dict(xaxis_title=\"R\", yaxis_title=\"G\", zaxis_title=\"B\", aspectmode=\"cube\"),\n",
    "            row=(i - 1) // 2 + 1,\n",
    "            col=(i - 1) % 2 + 1,\n",
    "        )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # ==========================================\n",
    "    # 定量的な考察の出力\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"【定量分析による考察】\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for result in analysis_results:\n",
    "        print(f\"\\n■ {result['pair']} ({result['count']}件)\")\n",
    "        print(\n",
    "            f\"  ├─ 誤分類サンプル → 元クラス重心までの距離: {result['dist_to_own']:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  ├─ 誤分類サンプル → 混同先クラス重心までの距離: {result['dist_to_pred']:.2f}\"\n",
    "        )\n",
    "        print(f\"  ├─ 2クラス間の重心距離 (分離度): {result['class_separation']:.2f}\")\n",
    "        print(\n",
    "            f\"  └─ 誤分類サンプルの分散: {result['wrong_variance']:.2f} (正解: {result['correct_variance']:.2f})\"\n",
    "        )\n",
    "\n",
    "        # 考察の自動判定\n",
    "        if result[\"dist_to_pred\"] < result[\"dist_to_own\"]:\n",
    "            print(\n",
    "                f\"  → 🔴 誤分類サンプルは混同先クラスに近い → RGBが誤分類の原因の可能性\"\n",
    "            )\n",
    "        elif result[\"dist_to_own\"] < result[\"class_separation\"] * 0.3:\n",
    "            print(\n",
    "                f\"  → 🟢 誤分類サンプルは元クラス内に留まっている → RGB以外の要因が原因\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  → 🟡 誤分類サンプルは両クラスの中間領域 → クラス境界が曖昧\")\n",
    "\n",
    "    # 全体的な考察\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"【RGB空間での分析まとめ】\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    rgb_caused = sum(1 for r in analysis_results if r[\"closer_to\"] == \"pred_class\")\n",
    "    total = len(analysis_results)\n",
    "\n",
    "    if total > 0:\n",
    "        rgb_ratio = rgb_caused / total\n",
    "        print(\n",
    "            f\"\\n• 誤分類がRGBに起因する可能性が高いペア: {rgb_caused}/{total} ({rgb_ratio * 100:.0f}%)\"\n",
    "        )\n",
    "\n",
    "        if rgb_ratio > 0.5:\n",
    "            print(\n",
    "                \"\\n📊 結論: 誤分類の多くはRGB（色）の類似性に起因している可能性が高い\"\n",
    "            )\n",
    "            print(\"   → 色が似ているクラス間での誤分類が多い\")\n",
    "            print(\"   → 対策: 色以外の特徴（テクスチャ、エッジ）を強調する前処理や、\")\n",
    "            print(\"           Data Augmentation（色変換）が有効かもしれない\")\n",
    "        else:\n",
    "            print(\"\\n📊 結論: 誤分類はRGBの類似性だけでは説明できない\")\n",
    "            print(\"   → 誤分類サンプルは元のクラスと同じRGB領域に存在している\")\n",
    "            print(\"   → RGB平均値だけでは捉えられない特徴が分類に重要\")\n",
    "            print(\"   → 考えられる要因:\")\n",
    "            print(\"     • テクスチャ（繰り返しパターン）\")\n",
    "            print(\"     • エッジ・輪郭の形状\")\n",
    "            print(\"     • 空間的な色の配置（画像の上下左右での色の違い）\")\n",
    "            print(\"     • 局所的な特徴（RGB平均では消えてしまう）\")\n",
    "\n",
    "\n",
    "# 実行\n",
    "plot_confusion_pairs_rgb(x_test, y_test, pred_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671446eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rgb_distribution_detail(x_test, y_test, pred_labels, class_names):\n",
    "    \"\"\"\n",
    "    より詳細なRGB分析：分散、ヒストグラム、空間的分布\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    incorrect_mask = pred_labels != y_test\n",
    "    correct_mask = ~incorrect_mask\n",
    "\n",
    "    # RGB平均と分散\n",
    "    rgb_means = np.mean(x_test, axis=(1, 2)) * 255\n",
    "    rgb_stds = np.std(x_test, axis=(1, 2)) * 255  # 画像内のRGB分散\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. 誤分類vs正解の RGB分散比較\n",
    "    # ==========================================\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[\n",
    "            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "        ],\n",
    "        subplot_titles=[\n",
    "            \"Red: 平均 vs 分散\",\n",
    "            \"Green: 平均 vs 分散\",\n",
    "            \"Blue: 平均 vs 分散\",\n",
    "            \"RGB分散の合計 (正解 vs 誤分類)\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    channel_names = [\"Red\", \"Green\", \"Blue\"]\n",
    "\n",
    "    for ch, name in enumerate(channel_names):\n",
    "        row = (ch // 2) + 1\n",
    "        col = (ch % 2) + 1\n",
    "\n",
    "        # 正解サンプル\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rgb_means[correct_mask, ch],\n",
    "                y=rgb_stds[correct_mask, ch],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=3, color=\"blue\", opacity=0.3),\n",
    "                name=\"正解\",\n",
    "                showlegend=(ch == 0),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        # 誤分類サンプル\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rgb_means[incorrect_mask, ch],\n",
    "                y=rgb_stds[incorrect_mask, ch],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=6, color=\"red\", opacity=0.7),\n",
    "                name=\"誤分類\",\n",
    "                showlegend=(ch == 0),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "    # RGB分散の合計\n",
    "    total_std_correct = np.sum(rgb_stds[correct_mask], axis=1)\n",
    "    total_std_incorrect = np.sum(rgb_stds[incorrect_mask], axis=1)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=total_std_correct,\n",
    "            name=\"正解\",\n",
    "            opacity=0.6,\n",
    "            marker_color=\"blue\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=total_std_incorrect,\n",
    "            name=\"誤分類\",\n",
    "            opacity=0.6,\n",
    "            marker_color=\"red\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"RGB平均 vs RGB分散（画像内のばらつき）\",\n",
    "        height=700,\n",
    "        width=900,\n",
    "        barmode=\"overlay\",\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. 統計的検定\n",
    "    # ==========================================\n",
    "    from scipy import stats\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"【統計的分析】誤分類サンプルの特徴\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # RGB平均の比較\n",
    "    print(\"\\n■ RGB平均値の比較 (t検定)\")\n",
    "    for ch, name in enumerate(channel_names):\n",
    "        correct_vals = rgb_means[correct_mask, ch]\n",
    "        incorrect_vals = rgb_means[incorrect_mask, ch]\n",
    "        t_stat, p_val = stats.ttest_ind(correct_vals, incorrect_vals)\n",
    "        sig = \"有意差あり ✓\" if p_val < 0.05 else \"有意差なし\"\n",
    "        print(\n",
    "            f\"  {name}: 正解={np.mean(correct_vals):.1f}±{np.std(correct_vals):.1f}, \"\n",
    "            f\"誤分類={np.mean(incorrect_vals):.1f}±{np.std(incorrect_vals):.1f} (p={p_val:.4f}) {sig}\"\n",
    "        )\n",
    "\n",
    "    # RGB分散の比較\n",
    "    print(\"\\n■ RGB分散（画像内ばらつき）の比較\")\n",
    "    for ch, name in enumerate(channel_names):\n",
    "        correct_vals = rgb_stds[correct_mask, ch]\n",
    "        incorrect_vals = rgb_stds[incorrect_mask, ch]\n",
    "        t_stat, p_val = stats.ttest_ind(correct_vals, incorrect_vals)\n",
    "        sig = \"有意差あり ✓\" if p_val < 0.05 else \"有意差なし\"\n",
    "        diff = np.mean(incorrect_vals) - np.mean(correct_vals)\n",
    "        direction = \"高い↑\" if diff > 0 else \"低い↓\"\n",
    "        print(\n",
    "            f\"  {name}: 誤分類は正解より分散が{direction} (差={diff:+.2f}, p={p_val:.4f}) {sig}\"\n",
    "        )\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. 空間的なRGB分布（上下左右）\n",
    "    # ==========================================\n",
    "    print(\"\\n■ 空間的なRGB分布の分析\")\n",
    "\n",
    "    # 画像を上半分と下半分に分けて比較\n",
    "    h = x_test.shape[1] // 2\n",
    "    top_mean = np.mean(x_test[:, :h, :, :], axis=(1, 2)) * 255\n",
    "    bottom_mean = np.mean(x_test[:, h:, :, :], axis=(1, 2)) * 255\n",
    "    vertical_diff = top_mean - bottom_mean  # 上下の色の違い\n",
    "\n",
    "    # 誤分類サンプルは上下の色差が大きい/小さい？\n",
    "    correct_v_diff = np.mean(np.abs(vertical_diff[correct_mask]), axis=1)\n",
    "    incorrect_v_diff = np.mean(np.abs(vertical_diff[incorrect_mask]), axis=1)\n",
    "\n",
    "    t_stat, p_val = stats.ttest_ind(correct_v_diff, incorrect_v_diff)\n",
    "    print(\n",
    "        f\"  上下の色差: 正解={np.mean(correct_v_diff):.2f}, 誤分類={np.mean(incorrect_v_diff):.2f} (p={p_val:.4f})\"\n",
    "    )\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. 考察のまとめ\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"【考察】\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # RGB平均に有意差があるか\n",
    "    rgb_mean_significant = False\n",
    "    for ch in range(3):\n",
    "        _, p_val = stats.ttest_ind(\n",
    "            rgb_means[correct_mask, ch], rgb_means[incorrect_mask, ch]\n",
    "        )\n",
    "        if p_val < 0.05:\n",
    "            rgb_mean_significant = True\n",
    "            break\n",
    "\n",
    "    # RGB分散に有意差があるか\n",
    "    rgb_std_significant = False\n",
    "    for ch in range(3):\n",
    "        _, p_val = stats.ttest_ind(\n",
    "            rgb_stds[correct_mask, ch], rgb_stds[incorrect_mask, ch]\n",
    "        )\n",
    "        if p_val < 0.05:\n",
    "            rgb_std_significant = True\n",
    "            break\n",
    "\n",
    "    if not rgb_mean_significant and not rgb_std_significant:\n",
    "        print(\"\\n🟢 誤分類サンプルと正解サンプルのRGB特性に統計的な有意差がない\")\n",
    "        print(\"   → RGBの平均値・分散だけでは誤分類を説明できない\")\n",
    "        print(\n",
    "            \"   → モデルはRGB以外の特徴（テクスチャ、形状、エッジ等）で判断している可能性が高い\"\n",
    "        )\n",
    "    elif rgb_mean_significant:\n",
    "        print(\"\\n🔴 誤分類サンプルはRGB平均値に特徴がある\")\n",
    "        print(\"   → 特定の色域で誤分類が起きやすい\")\n",
    "    elif rgb_std_significant:\n",
    "        print(\"\\n🟡 誤分類サンプルはRGB分散（画像内のばらつき）に特徴がある\")\n",
    "        print(\"   → 均一な色の画像 or 色が複雑な画像で誤分類しやすい可能性\")\n",
    "\n",
    "\n",
    "# 実行\n",
    "analyze_rgb_distribution_detail(x_test, y_test, pred_labels, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e1039",
   "metadata": {},
   "source": [
    "## 📊 RGB分析のまとめ（研究発表用）\n",
    "\n",
    "### 分析目的\n",
    "ResNetの誤分類が**RGB（色情報）に起因するか**を検証し、モデルの判断根拠を考察する。\n",
    "\n",
    "---\n",
    "\n",
    "### 主要な誤分類ペアの分析結果\n",
    "\n",
    "| 誤分類ペア | 件数 | 誤分類サンプルの位置 | 原因の推定 |\n",
    "|-----------|------|---------------------|-----------|\n",
    "| Industrial → River | 162 | 元クラス内 | **RGB以外**（テクスチャ等） |\n",
    "| HerbaceousVegetation → PermanentCrop | 117 | 混同先クラスに近い | **RGB（色の類似性）** |\n",
    "| Highway → River | 96 | 中間領域 | クラス境界が曖昧 |\n",
    "| **SeaLake → Pasture** | 90 | 混同先クラスに近い | **RGB（緑色の誤認）** |\n",
    "\n",
    "---\n",
    "\n",
    "### 考察\n",
    "\n",
    "#### 1. SeaLake → Pasture の誤分類（RGBが原因）\n",
    "- 誤分類サンプルは**元クラス（SeaLake）の重心から68.99離れている**\n",
    "- 一方、**混同先（Pasture）の重心には23.60と近い**\n",
    "- **分散が306.47**と大きく、SeaLakeの正解サンプル（分散50.21）とは明らかに異なる色分布\n",
    "- → **緑色成分が強い湖沼画像が、牧草地と誤認された**と考えられる\n",
    "- → 藻類の繁殖した湖や、浅い水域が緑がかって見えるケースが該当\n",
    "\n",
    "#### 2. Industrial → River の誤分類（RGB以外が原因）\n",
    "- 誤分類サンプルは**元クラス内に留まっている**（距離7.97）\n",
    "- RGB空間では正解サンプルと区別がつかない\n",
    "- → **テクスチャ（建物のパターン vs 川の流れ）** や **形状** で判断を誤った可能性\n",
    "\n",
    "#### 3. Highway → River の誤分類（境界が曖昧）\n",
    "- 2クラス間の重心距離が11.30と**非常に近い**\n",
    "- → Highway と River は**RGB空間で重なっている**（灰色系の類似した色域）\n",
    "- → 色だけでは本質的に区別が困難なクラスペア\n",
    "\n",
    "---\n",
    "\n",
    "### 結論\n",
    "\n",
    "| 観点 | 結果 |\n",
    "|-----|------|\n",
    "| RGBが原因の誤分類 | **2/4ペア（50%）** |\n",
    "| RGB以外が原因 | 1/4ペア（25%） |\n",
    "| 境界が曖昧 | 1/4ペア（25%） |\n",
    "\n",
    "**ResNetの誤分類は、単純にRGBの類似性だけでは説明できない。**\n",
    "\n",
    "ただし、**SeaLake → Pasture** のように、**色（特に緑色成分）が直接的な原因となるケースも存在する**。\n",
    "これは、湖沼が藻類等で緑色に見える場合に、モデルが「緑=植物」と判断してしまうことを示唆している。\n",
    "\n",
    "---\n",
    "\n",
    "### 今後の改善案\n",
    "1. **色に依存しにくいData Augmentation**: 色相変換、彩度変換を学習時に適用\n",
    "2. **テクスチャ特徴の強化**: ガボールフィルタやLBP特徴量の追加\n",
    "3. **マルチスペクトル情報の活用**: RGB以外のバンド（NIR等）を使用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
