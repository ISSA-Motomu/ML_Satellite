{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b978dad",
   "metadata": {},
   "source": [
    "### ResNetの実装\n",
    "ResNetの基本式 $y = F(x) + x$\n",
    "ここで $F(x)$ は残差関数\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{ReLU}(F(x) + x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9eb64",
   "metadata": {},
   "source": [
    "### BatchNormalization()の中身について\n",
    "> ReLUにそのまま通してしまったら負数の情報が全て0になってしまうので、標準化を行う\n",
    "> $$ \\hat{x} = \\frac{x - \\mu (\\text{平均})}{\\sigma (\\text{標準偏差})} $$\n",
    "- 入ってきたデータ（ミニバッチ）に対して**標準化「平均を0,分散を1」**\n",
    "- データから平均値を引くことで中心を0としている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ea8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tensorflow\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Add,BatchNormalization\n",
    "\n",
    "def residual_block(input_tensor, num_filters,strides=1):\n",
    "    \"\"\"\n",
    "    入力と出力のサイズが変わらない、基本的な残差ブロック\n",
    "    残差：Residual\n",
    "    Output = f(Input)+Input\n",
    "        ここでFは2回の畳み込みとReLU活性化\n",
    "\n",
    "    Skip Connection\n",
    "        残差分だけを学習することで、層が深くなっても勾配損失問題を防ぐ\n",
    "    \n",
    "    Conv\n",
    "        畳み込みをすると通常は画面の端っこが削れて小さくなるが、padding='same'を指定して周りに余白をつける\n",
    "        そうすると入力と出力のサイズが変わらなくなる\n",
    "\n",
    "    Add()([x,shortcut])\n",
    "        KerasのFunctional APIの書き方\n",
    "            ➀足し算マシーン(Addレイヤー)の生成、ここでAddは単純に足し算をするだけなので()の中に複雑な設定が必要ない\n",
    "            ➁足し算マシーンに[x,shortcut]というリストの形状にして足し算を実行する\n",
    "\n",
    "    Strides\n",
    "        畳み込みの移動幅を指定するパラメータ\n",
    "        1なら通常通り1pxずつ、2なら2pxずつ移動する(1個飛ばし)ので、出力サイズが半分になる\n",
    "\n",
    "    num_filters\n",
    "        各num_filterは畳み込み層で検知した特徴マップを持っている\n",
    "        例えば浅い層のフィルターではエッジ検出、深い層ではより複雑なパターンを検出する\n",
    "            エッジ：隣り合う画素(ピクセル)が急激に変化している部分\n",
    "    \"\"\"\n",
    "    shortcut = input_tensor\n",
    "\n",
    "    # 畳み込み一回目\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same',strides=strides)(input_tensor)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 畳み込み二回目\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n",
    "    x= BatchNormalization()(x)\n",
    "\n",
    "    if strides > 1 or input_tensor.shape[-1] != num_filters:\n",
    "        \"\"\"\n",
    "        サイズが変わる、またはチャンネル数が変わるとき\n",
    "        .shape[-1]は最後の要素、つまりchannnel\n",
    "        \"\"\"\n",
    "        shortcut = Conv2D(num_filters, (1, 1), padding='same',strides=strides)(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
    "\n",
    "\n",
    "def build_simple_resnet(input_shape=(128, 128, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    ResNetの簡易モデルの作成\n",
    "        画像分類タスクでは、画像の特徴の身を知りたいので畳み込み層のエンコードしか必要ない\n",
    "        セグメンテーションは位置情報も必要なのでdecodeも必要になる\n",
    "        ResNet18の18は層の数、50や152などがある\n",
    "\n",
    "    Projection(射影)\n",
    "            ➀入力の特徴マップのサイズを変換する\n",
    "            ➁フィルター数(channel数)を変更する\n",
    "\n",
    "    GlobalAveragePooling2D\n",
    "        Strides=1は画像サイズを変えないまま特徴をしっかり抽出する\n",
    "\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    x = residual_block(inputs, num_filters=64, strides=1)\n",
    "\n",
    "    x = residual_block(\n",
    "        x, num_filters=64, strides=1\n",
    "    )  # サイズを変えないまま特徴をしっかり見極める\n",
    "    x = residual_block(x, num_filters=64, strides=1)\n",
    "\n",
    "    x = residual_block(\n",
    "        x, num_filters=128, strides=2\n",
    "    )  # サイズを半分にする(Projection Block)\n",
    "    x = residual_block(x, num_filters=128, strides=1)\n",
    "\n",
    "    # --- 出口 (Output Layers) ---\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # 最終判定 (10クラス分類)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239de01b",
   "metadata": {},
   "source": [
    "### 1. GlobalAveragePooling2D　(GAP)\n",
    "**位置情報は捨てて、特徴の強さだけを残す層**\n",
    "> 一枚の特徴マップの全画素の平均を計算して、有効な数字であればその特徴が存在すると考える\n",
    "  - 入力: $(Batch, 8, 8, 128)$ → 高さ8, 幅8, チャンネル128\n",
    "  - 出力: $(Batch, 128)$ → 長さ128のベクトル\n",
    "\n",
    "### 2. Dense （全結合層）\n",
    "  - 最後のDense層のユニット数は、**分類したいクラス数**（EuroSATなら10個）と一致させる\n",
    "  - 最後に`Softmax`関数を通すことで、出力を確率（合計すると100%）に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb41e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ダウンロードと読み込みが完了しました！\n",
      "クラス名: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "学習データ数: 21600\n",
      "検証データ数: 2700\n",
      "テストデータ数: 2700\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "\n",
    "# データを保存するディレクトリ (先ほど作った data フォルダを指定)\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "\"\"\"\n",
    "tfds.load()でdatasetをダウンロードして読みこむ\n",
    "    タプルのデータセットとinfoを返す\n",
    "        eurosatのRGBバージョンを使用する\n",
    "        as_supervised=True で(画像, ラベル)のタプルで返す\n",
    "        split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'] で学習・検証・テストに分割\n",
    "\n",
    "\"\"\"\n",
    "(train_ds, val_ds, test_ds), info = tfds.load(\n",
    "    \"eurosat/rgb\",\n",
    "    split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "    data_dir=DATA_DIR,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "print(\"ダウンロードと読み込みが完了しました！\")\n",
    "print(f\"クラス名: {info.features['label'].names}\")\n",
    "print(f\"学習データ数: {len(train_ds)}\")\n",
    "print(f\"検証データ数: {len(val_ds)}\")\n",
    "print(f\"テストデータ数: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73845dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在のポリシー: float16\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(f\"現在のポリシー: {policy.compute_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106012a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: (32, 64, 64, 3)\n",
      "Label Batch Shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Setting constants\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def preprocess_data(image, label):\n",
    "    \"\"\"\n",
    "    画像データの前処理を行う関数\n",
    "        1. サイズを確実に合わせる (リサイズ)\n",
    "            - `tf.image.resize()`はTensorflowのimageモジュールの中にある関数\n",
    "            - interpolation 補間：いい感じに新しい色を計算して埋める\n",
    "            > 有限要素法のバイリニア補間\n",
    "                すべての画素に対して、周囲の整数格子点の値を使って線形補間を行う方法\n",
    "\n",
    "        2. 正規化 (Normalization)\n",
    "            - 画像のRGB各チャンネルの値を0から1の範囲にスケーリングする\n",
    "                - 正規化しないと計算量が膨大になる\n",
    "\n",
    "        .take(n)\n",
    "            - データセットから最初のn個の要素を取得するメソッド\n",
    "    \"\"\"\n",
    "    image = tf.image.resize(\n",
    "        image, (IMG_SIZE, IMG_SIZE)\n",
    "    )  # 1.サイズを確実に合わせる(リサイズ)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # 2.正規化(Normalization)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# --- パイプラインの構築 ---\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_batches = (\n",
    "    train_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .shuffle(buffer_size=1000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_batches = (\n",
    "    val_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 確認\n",
    "for img_batch, label_batch in train_batches.take(1):\n",
    "    print(f\"Image Batch Shape: {img_batch.shape}\")\n",
    "    print(f\"Label Batch Shape: {label_batch.shape}\")\n",
    "\n",
    "    # Image: (32, 64, 64, 3) -> 32枚, 64x64ピクセル, 3チャンネル(RGB)\n",
    "    # Label: (32,) -> 32個の正解ラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf5151",
   "metadata": {},
   "source": [
    "### データパイプライン DataPipeLine\n",
    "> データパイプラインとは、データをストレージから読み出し、計算モデルが処理可能なテンソルへと変換し、GPUやTPUへ供給する一連の処理工程、およびそれを実装したソフトウェアアーキテクチャを指す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shapeは (64, 64, 3), クラス数は 10\n",
    "model = build_simple_resnet(input_shape=(64, 64, 3), num_classes=10)\n",
    "\n",
    "# --- コンパイル (学習ルールの設定) ---\n",
    "model.compile(\n",
    "    # AIの「間違いの修正方法」を指定します。Adamは最も一般的で優秀な修正担当者です。\n",
    "    optimizer=\"adam\",  # Optimizer (最適化アルゴリズム): 'adam'\n",
    "    # Loss (損失関数): 'SparseCategoricalCrossentropy'\n",
    "    # from_logits=True は、AIの生の出力を確率に変換してから計算しろ、という指示です。\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    # Metrics:'accuracy'\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# モデルの設計図確認\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9678ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 学習の実行 ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:919\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    913\u001b[39m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[32m    914\u001b[39m   filtered_flat_args = (\n\u001b[32m    915\u001b[39m       \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.unpack_inputs(\n\u001b[32m    916\u001b[39m           bound_args\n\u001b[32m    917\u001b[39m       )\n\u001b[32m    918\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    920\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[32m    925\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 学習の実行 ---\n",
    "history = model.fit(train_batches, validation_data=val_batches, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1690ad5",
   "metadata": {},
   "source": [
    "###　Result\n",
    "- 自分のPCのCPU,Core i7だと90分かかった![Result of CPU Learning](images/122101.png)\n",
    "<br>\n",
    "- AI工房のGPUサーバーを使うと40秒程度で5epoch終了->**135倍!!のスピード**![Result of GPU Learning](images/122102.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- テストデータの準備と評価 ---\n",
    "\n",
    "# テストデータにも同様の前処理（リサイズ・正規化）とバッチ化を適用\n",
    "test_batches = (\n",
    "    test_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "# モデルの評価 (未学習データでの性能確認)\n",
    "print(\"テストデータで評価を実行します...\")\n",
    "test_loss, test_acc = model.evaluate(test_batches)\n",
    "\n",
    "print(f\"\\nテストデータの正解率: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# クラス名を取得\n",
    "class_names = info.features[\"label\"].names\n",
    "\n",
    "\n",
    "def plot_predictions(dataset, model, num_images=9):\n",
    "    \"\"\"\n",
    "    テストデータに対して推論を行い、画像と予測結果を表示する関数\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    # データセットから1バッチ(32枚)だけ取り出す\n",
    "    for images, labels in dataset.take(1):\n",
    "        # 推論の実行 (確率が出力される)\n",
    "        predictions = model.predict(images)\n",
    "        # 最も確率が高いクラスのインデックスを取得\n",
    "        pred_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # 指定枚数分だけ表示\n",
    "        for i in range(min(num_images, len(images))):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i])\n",
    "\n",
    "            # ラベル名の取得\n",
    "            true_label = class_names[labels[i]]\n",
    "            pred_label = class_names[pred_indices[i]]\n",
    "            confidence = 100 * np.max(predictions[i])\n",
    "\n",
    "            # 正解なら青、不正解なら赤でタイトルを表示\n",
    "            color = \"blue\" if labels[i] == pred_indices[i] else \"red\"\n",
    "\n",
    "            plt.title(\n",
    "                f\"Pred: {pred_label} ({confidence:.1f}%)\\nTrue: {true_label}\",\n",
    "                color=color,\n",
    "            )\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 可視化の実行\n",
    "print(\"推論結果の可視化:\")\n",
    "plot_predictions(test_batches, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba36e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# --- 分析用データの準備 ---\n",
    "print(\"詳細なエラー分析を実行します...\")\n",
    "\n",
    "# 全テストデータの予測と正解ラベルの取得\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "# データセットから全データを取得\n",
    "for img_batch, label_batch in test_batches:\n",
    "    all_images.append(img_batch.numpy())\n",
    "    all_labels.append(label_batch.numpy())\n",
    "\n",
    "x_test = np.concatenate(all_images)\n",
    "y_test = np.concatenate(all_labels)\n",
    "\n",
    "# 推論実行 (バッチ処理しているので高速)\n",
    "predictions = model.predict(x_test, verbose=0)\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "max_probs = np.max(predictions, axis=1)\n",
    "\n",
    "# --- 1. 混同行列 (Confusion Matrix) ---\n",
    "conf_matrix = confusion_matrix(y_test, pred_labels)\n",
    "\n",
    "# --- 2. 「自信満々に間違えた」データの抽出 ---\n",
    "incorrect_indices = np.where(pred_labels != y_test)[0]\n",
    "# 予測確率(自信)が高い順にソートしてトップを取得\n",
    "sorted_incorrect_indices = incorrect_indices[\n",
    "    np.argsort(max_probs[incorrect_indices])[::-1]\n",
    "]\n",
    "\n",
    "\n",
    "# --- 3. Grad-CAM (判断根拠の可視化) ---\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    if len(img_array.shape) == 3:\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "\n",
    "def find_target_layer(model):\n",
    "    for layer in reversed(model.layers):\n",
    "        if len(layer.output_shape) == 4:\n",
    "            return layer.name\n",
    "    return None\n",
    "\n",
    "\n",
    "target_layer_name = find_target_layer(model)\n",
    "\n",
    "# --- 可視化プロット ---\n",
    "plt.figure(figsize=(20, 10))\n",
    "gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 0.5, 0.5])\n",
    "\n",
    "# 左側: 混同行列\n",
    "ax_cm = plt.subplot(gs[:, :2])\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    ax=ax_cm,\n",
    ")\n",
    "ax_cm.set_title(\"Confusion Matrix\", fontsize=16)\n",
    "ax_cm.set_ylabel(\"True Label\", fontsize=14)\n",
    "ax_cm.set_xlabel(\"Predicted Label\", fontsize=14)\n",
    "\n",
    "# 右側: 自信満々に間違えた画像 Top 4\n",
    "num_display = 4\n",
    "for i in range(min(num_display, len(sorted_incorrect_indices))):\n",
    "    idx = sorted_incorrect_indices[i]\n",
    "    img = x_test[idx]\n",
    "    true_lb = class_names[y_test[idx]]\n",
    "    pred_lb = class_names[pred_labels[idx]]\n",
    "    conf = max_probs[idx]\n",
    "\n",
    "    # Grad-CAM ヒートマップ\n",
    "    heatmap = make_gradcam_heatmap(\n",
    "        img, model, target_layer_name, pred_index=pred_labels[idx]\n",
    "    )\n",
    "\n",
    "    # リサイズと重ね合わせ表示\n",
    "    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], (IMG_SIZE, IMG_SIZE))\n",
    "    heatmap_resized = tf.squeeze(heatmap_resized).numpy()\n",
    "\n",
    "    # グリッド配置 (右半分の領域を使う)\n",
    "    row = i // 2\n",
    "    col = 2 + (i % 2)\n",
    "    ax_img = plt.subplot(gs[row, col])\n",
    "\n",
    "    ax_img.imshow(img)\n",
    "    ax_img.imshow(heatmap_resized, alpha=0.5, cmap=\"jet\")\n",
    "\n",
    "    # 枠線の色 (赤: 間違い)\n",
    "    for spine in ax_img.spines.values():\n",
    "        spine.set_edgecolor(\"red\")\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    ax_img.set_title(\n",
    "        f\"True: {true_lb}\\nPred: {pred_lb}\\nConf: {conf:.1%}\",\n",
    "        fontsize=12,\n",
    "        color=\"red\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax_img.set_xticks([])\n",
    "    ax_img.set_yticks([])\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Error Analysis & Grad-CAM (Target Layer: {target_layer_name})\", fontsize=20\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 考察コメントの例示\n",
    "print(\"\\n=== 考察コメントの例 ===\")\n",
    "print(\n",
    "    \"Grad-CAMのヒートマップ（赤色部分）を確認することで、モデルが画像のどこを見て判断したかが分かります。\"\n",
    ")\n",
    "print(\n",
    "    \"これを用いて、『形ではなく色だけで判断してしまっている』などの誤答原因を分析できます。\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
