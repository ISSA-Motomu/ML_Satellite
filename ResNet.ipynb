{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b978dad",
   "metadata": {},
   "source": [
    "### ResNetã®å®Ÿè£…\n",
    "ResNetã®åŸºæœ¬å¼ $y = F(x) + x$\n",
    "ã“ã“ã§ $F(x)$ ã¯æ®‹å·®é–¢æ•°\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{ReLU}(F(x) + x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9eb64",
   "metadata": {},
   "source": [
    "### BatchNormalization()ã®ä¸­èº«ã«ã¤ã„ã¦\n",
    "> ReLUã«ãã®ã¾ã¾é€šã—ã¦ã—ã¾ã£ãŸã‚‰è² æ•°ã®æƒ…å ±ãŒå…¨ã¦0ã«ãªã£ã¦ã—ã¾ã†ã®ã§ã€æ¨™æº–åŒ–ã‚’è¡Œã†\n",
    "> $$ \\hat{x} = \\frac{x - \\mu (\\text{å¹³å‡})}{\\sigma (\\text{æ¨™æº–åå·®})} $$\n",
    "- å…¥ã£ã¦ããŸãƒ‡ãƒ¼ã‚¿ï¼ˆãƒŸãƒ‹ãƒãƒƒãƒï¼‰ã«å¯¾ã—ã¦**æ¨™æº–åŒ–ã€Œå¹³å‡ã‚’0,åˆ†æ•£ã‚’1ã€**\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¹³å‡å€¤ã‚’å¼•ãã“ã¨ã§ä¸­å¿ƒã‚’0ã¨ã—ã¦ã„ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ea8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tensorflow\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Add, BatchNormalization\n",
    "\n",
    "\n",
    "def residual_block(input_tensor, num_filters, strides=1):\n",
    "    \"\"\"\n",
    "    å…¥åŠ›ã¨å‡ºåŠ›ã®ã‚µã‚¤ã‚ºãŒå¤‰ã‚ã‚‰ãªã„ã€åŸºæœ¬çš„ãªæ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    æ®‹å·®ï¼šResidual\n",
    "    Output = f(Input)+Input\n",
    "        ã“ã“ã§Fã¯2å›ã®ç•³ã¿è¾¼ã¿ã¨ReLUæ´»æ€§åŒ–\n",
    "\n",
    "    Skip Connection\n",
    "        æ®‹å·®åˆ†ã ã‘ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€å±¤ãŒæ·±ããªã£ã¦ã‚‚å‹¾é…æå¤±å•é¡Œã‚’é˜²ã\n",
    "\n",
    "    Conv\n",
    "        ç•³ã¿è¾¼ã¿ã‚’ã™ã‚‹ã¨é€šå¸¸ã¯ç”»é¢ã®ç«¯ã£ã“ãŒå‰Šã‚Œã¦å°ã•ããªã‚‹ãŒã€padding='same'ã‚’æŒ‡å®šã—ã¦å‘¨ã‚Šã«ä½™ç™½ã‚’ã¤ã‘ã‚‹\n",
    "        ãã†ã™ã‚‹ã¨å…¥åŠ›ã¨å‡ºåŠ›ã®ã‚µã‚¤ã‚ºãŒå¤‰ã‚ã‚‰ãªããªã‚‹\n",
    "\n",
    "    Add()([x,shortcut])\n",
    "        Kerasã®Functional APIã®æ›¸ãæ–¹\n",
    "            â€è¶³ã—ç®—ãƒã‚·ãƒ¼ãƒ³(Addãƒ¬ã‚¤ãƒ¤ãƒ¼)ã®ç”Ÿæˆã€ã“ã“ã§Addã¯å˜ç´”ã«è¶³ã—ç®—ã‚’ã™ã‚‹ã ã‘ãªã®ã§()ã®ä¸­ã«è¤‡é›‘ãªè¨­å®šãŒå¿…è¦ãªã„\n",
    "            âè¶³ã—ç®—ãƒã‚·ãƒ¼ãƒ³ã«[x,shortcut]ã¨ã„ã†ãƒªã‚¹ãƒˆã®å½¢çŠ¶ã«ã—ã¦è¶³ã—ç®—ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "\n",
    "    Strides\n",
    "        ç•³ã¿è¾¼ã¿ã®ç§»å‹•å¹…ã‚’æŒ‡å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        1ãªã‚‰é€šå¸¸é€šã‚Š1pxãšã¤ã€2ãªã‚‰2pxãšã¤ç§»å‹•ã™ã‚‹(1å€‹é£›ã°ã—)ã®ã§ã€å‡ºåŠ›ã‚µã‚¤ã‚ºãŒåŠåˆ†ã«ãªã‚‹\n",
    "\n",
    "    num_filters\n",
    "        å„num_filterã¯ç•³ã¿è¾¼ã¿å±¤ã§æ¤œçŸ¥ã—ãŸç‰¹å¾´ãƒãƒƒãƒ—ã‚’æŒã£ã¦ã„ã‚‹\n",
    "        ä¾‹ãˆã°æµ…ã„å±¤ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§ã¯ã‚¨ãƒƒã‚¸æ¤œå‡ºã€æ·±ã„å±¤ã§ã¯ã‚ˆã‚Šè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã™ã‚‹\n",
    "            ã‚¨ãƒƒã‚¸ï¼šéš£ã‚Šåˆã†ç”»ç´ (ãƒ”ã‚¯ã‚»ãƒ«)ãŒæ€¥æ¿€ã«å¤‰åŒ–ã—ã¦ã„ã‚‹éƒ¨åˆ†\n",
    "    \"\"\"\n",
    "    shortcut = input_tensor\n",
    "\n",
    "    # ç•³ã¿è¾¼ã¿ä¸€å›ç›®\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\", strides=strides)(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # ç•³ã¿è¾¼ã¿äºŒå›ç›®\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if strides > 1 or input_tensor.shape[-1] != num_filters:\n",
    "        \"\"\"\n",
    "        ã‚µã‚¤ã‚ºãŒå¤‰ã‚ã‚‹ã€ã¾ãŸã¯ãƒãƒ£ãƒ³ãƒãƒ«æ•°ãŒå¤‰ã‚ã‚‹ã¨ã\n",
    "        .shape[-1]ã¯æœ€å¾Œã®è¦ç´ ã€ã¤ã¾ã‚Šchannnel\n",
    "        \"\"\"\n",
    "        shortcut = Conv2D(num_filters, (1, 1), padding=\"same\", strides=strides)(\n",
    "            shortcut\n",
    "        )\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
    "\n",
    "\n",
    "def build_simple_resnet(input_shape=(128, 128, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    ResNetã®ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "        ç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã¯ã€ç”»åƒã®ç‰¹å¾´ã®èº«ã‚’çŸ¥ã‚ŠãŸã„ã®ã§ç•³ã¿è¾¼ã¿å±¤ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã‹å¿…è¦ãªã„\n",
    "        ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ä½ç½®æƒ…å ±ã‚‚å¿…è¦ãªã®ã§decodeã‚‚å¿…è¦ã«ãªã‚‹\n",
    "        ResNet18ã®18ã¯å±¤ã®æ•°ã€50ã‚„152ãªã©ãŒã‚ã‚‹\n",
    "\n",
    "    Projection(å°„å½±)\n",
    "            â€å…¥åŠ›ã®ç‰¹å¾´ãƒãƒƒãƒ—ã®ã‚µã‚¤ã‚ºã‚’å¤‰æ›ã™ã‚‹\n",
    "            âãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ•°(channelæ•°)ã‚’å¤‰æ›´ã™ã‚‹\n",
    "\n",
    "    GlobalAveragePooling2D\n",
    "        Strides=1ã¯ç”»åƒã‚µã‚¤ã‚ºã‚’å¤‰ãˆãªã„ã¾ã¾ç‰¹å¾´ã‚’ã—ã£ã‹ã‚ŠæŠ½å‡ºã™ã‚‹\n",
    "\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    x = residual_block(inputs, num_filters=64, strides=1)\n",
    "\n",
    "    x = residual_block(\n",
    "        x, num_filters=64, strides=1\n",
    "    )  # ã‚µã‚¤ã‚ºã‚’å¤‰ãˆãªã„ã¾ã¾ç‰¹å¾´ã‚’ã—ã£ã‹ã‚Šè¦‹æ¥µã‚ã‚‹\n",
    "    x = residual_block(x, num_filters=64, strides=1)\n",
    "\n",
    "    x = residual_block(\n",
    "        x, num_filters=128, strides=2\n",
    "    )  # ã‚µã‚¤ã‚ºã‚’åŠåˆ†ã«ã™ã‚‹(Projection Block)\n",
    "    x = residual_block(x, num_filters=128, strides=1)\n",
    "\n",
    "    # --- å‡ºå£ (Output Layers) ---\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # æœ€çµ‚åˆ¤å®š (10ã‚¯ãƒ©ã‚¹åˆ†é¡)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239de01b",
   "metadata": {},
   "source": [
    "### 1. GlobalAveragePooling2Dã€€(GAP)\n",
    "**ä½ç½®æƒ…å ±ã¯æ¨ã¦ã¦ã€ç‰¹å¾´ã®å¼·ã•ã ã‘ã‚’æ®‹ã™å±¤**\n",
    "> ä¸€æšã®ç‰¹å¾´ãƒãƒƒãƒ—ã®å…¨ç”»ç´ ã®å¹³å‡ã‚’è¨ˆç®—ã—ã¦ã€æœ‰åŠ¹ãªæ•°å­—ã§ã‚ã‚Œã°ãã®ç‰¹å¾´ãŒå­˜åœ¨ã™ã‚‹ã¨è€ƒãˆã‚‹\n",
    "  - å…¥åŠ›: $(Batch, 8, 8, 128)$ â†’ é«˜ã•8, å¹…8, ãƒãƒ£ãƒ³ãƒãƒ«128\n",
    "  - å‡ºåŠ›: $(Batch, 128)$ â†’ é•·ã•128ã®ãƒ™ã‚¯ãƒˆãƒ«\n",
    "\n",
    "### 2. Dense ï¼ˆå…¨çµåˆå±¤ï¼‰\n",
    "  - æœ€å¾Œã®Denseå±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã¯ã€**åˆ†é¡ã—ãŸã„ã‚¯ãƒ©ã‚¹æ•°**ï¼ˆEuroSATãªã‚‰10å€‹ï¼‰ã¨ä¸€è‡´ã•ã›ã‚‹\n",
    "  - æœ€å¾Œã«`Softmax`é–¢æ•°ã‚’é€šã™ã“ã¨ã§ã€å‡ºåŠ›ã‚’ç¢ºç‡ï¼ˆåˆè¨ˆã™ã‚‹ã¨100%ï¼‰ã«å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb41e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n",
      "ã‚¯ãƒ©ã‚¹å: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
      "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: 21600\n",
      "æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ•°: 2700\n",
      "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: 2700\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (å…ˆã»ã©ä½œã£ãŸ data ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®š)\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "\"\"\"\n",
    "tfds.load()ã§datasetã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦èª­ã¿ã“ã‚€\n",
    "    ã‚¿ãƒ—ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨infoã‚’è¿”ã™\n",
    "        eurosatã®RGBãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹\n",
    "        as_supervised=True ã§(ç”»åƒ, ãƒ©ãƒ™ãƒ«)ã®ã‚¿ãƒ—ãƒ«ã§è¿”ã™\n",
    "        split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'] ã§å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã«åˆ†å‰²\n",
    "\n",
    "\"\"\"\n",
    "(train_ds, val_ds, test_ds), info = tfds.load(\n",
    "    \"eurosat/rgb\",\n",
    "    split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "    data_dir=DATA_DIR,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "print(\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(f\"ã‚¯ãƒ©ã‚¹å: {info.features['label'].names}\")\n",
    "print(f\"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_ds)}\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ•°: {len(val_ds)}\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73845dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼: float16\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(f\"ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼: {policy.compute_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106012a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Batch Shape: (32, 64, 64, 3)\n",
      "Label Batch Shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Setting constants\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 256  # AIå·¥æˆ¿ã®RTX 3090 + VRAM 24GB\n",
    "\n",
    "\n",
    "def preprocess_data(image, label):\n",
    "    \"\"\"\n",
    "    ç”»åƒãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’è¡Œã†é–¢æ•°\n",
    "        1. ã‚µã‚¤ã‚ºã‚’ç¢ºå®Ÿã«åˆã‚ã›ã‚‹ (ãƒªã‚µã‚¤ã‚º)\n",
    "            - `tf.image.resize()`ã¯Tensorflowã®imageãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä¸­ã«ã‚ã‚‹é–¢æ•°\n",
    "            - interpolation è£œé–“ï¼šã„ã„æ„Ÿã˜ã«æ–°ã—ã„è‰²ã‚’è¨ˆç®—ã—ã¦åŸ‹ã‚ã‚‹\n",
    "            > æœ‰é™è¦ç´ æ³•ã®ãƒã‚¤ãƒªãƒ‹ã‚¢è£œé–“\n",
    "                ã™ã¹ã¦ã®ç”»ç´ ã«å¯¾ã—ã¦ã€å‘¨å›²ã®æ•´æ•°æ ¼å­ç‚¹ã®å€¤ã‚’ä½¿ã£ã¦ç·šå½¢è£œé–“ã‚’è¡Œã†æ–¹æ³•\n",
    "\n",
    "        2. æ­£è¦åŒ– (Normalization)\n",
    "            - ç”»åƒã®RGBå„ãƒãƒ£ãƒ³ãƒãƒ«ã®å€¤ã‚’0ã‹ã‚‰1ã®ç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹\n",
    "                - æ­£è¦åŒ–ã—ãªã„ã¨è¨ˆç®—é‡ãŒè†¨å¤§ã«ãªã‚‹\n",
    "\n",
    "        .take(n)\n",
    "            - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æœ€åˆã®nå€‹ã®è¦ç´ ã‚’å–å¾—ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰\n",
    "    \"\"\"\n",
    "    image = tf.image.resize(\n",
    "        image, (IMG_SIZE, IMG_SIZE)\n",
    "    )  # 1.ã‚µã‚¤ã‚ºã‚’ç¢ºå®Ÿã«åˆã‚ã›ã‚‹(ãƒªã‚µã‚¤ã‚º)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # 2.æ­£è¦åŒ–(Normalization)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰ ---\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_batches = (\n",
    "    train_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .shuffle(buffer_size=1000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_batches = (\n",
    "    val_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ç¢ºèª\n",
    "for img_batch, label_batch in train_batches.take(1):\n",
    "    print(f\"Image Batch Shape: {img_batch.shape}\")\n",
    "    print(f\"Label Batch Shape: {label_batch.shape}\")\n",
    "\n",
    "    # Image: (32, 64, 64, 3) -> 32æš, 64x64ãƒ”ã‚¯ã‚»ãƒ«, 3ãƒãƒ£ãƒ³ãƒãƒ«(RGB)\n",
    "    # Label: (32,) -> 32å€‹ã®æ­£è§£ãƒ©ãƒ™ãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf5151",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ DataPipeLine\n",
    "> ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰èª­ã¿å‡ºã—ã€è¨ˆç®—ãƒ¢ãƒ‡ãƒ«ãŒå‡¦ç†å¯èƒ½ãªãƒ†ãƒ³ã‚½ãƒ«ã¸ã¨å¤‰æ›ã—ã€GPUã‚„TPUã¸ä¾›çµ¦ã™ã‚‹ä¸€é€£ã®å‡¦ç†å·¥ç¨‹ã€ãŠã‚ˆã³ãã‚Œã‚’å®Ÿè£…ã—ãŸã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æŒ‡ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shapeã¯ (64, 64, 3), ã‚¯ãƒ©ã‚¹æ•°ã¯ 10\n",
    "model = build_simple_resnet(input_shape=(64, 64, 3), num_classes=10)\n",
    "\n",
    "# --- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« (å­¦ç¿’ãƒ«ãƒ¼ãƒ«ã®è¨­å®š) ---\n",
    "model.compile(\n",
    "    # AIã®ã€Œé–“é•ã„ã®ä¿®æ­£æ–¹æ³•ã€ã‚’æŒ‡å®šã—ã¾ã™ã€‚Adamã¯æœ€ã‚‚ä¸€èˆ¬çš„ã§å„ªç§€ãªä¿®æ­£æ‹…å½“è€…ã§ã™ã€‚\n",
    "    optimizer=\"adam\",  # Optimizer (æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ): 'adam'\n",
    "    # Loss (æå¤±é–¢æ•°): 'SparseCategoricalCrossentropy'\n",
    "    # from_logits=True ã¯ã€AIã®ç”Ÿã®å‡ºåŠ›ã‚’ç¢ºç‡ã«å¤‰æ›ã—ã¦ã‹ã‚‰è¨ˆç®—ã—ã‚ã€ã¨ã„ã†æŒ‡ç¤ºã§ã™ã€‚\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    # Metrics:'accuracy'\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆå›³ç¢ºèª\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9678ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- å­¦ç¿’ã®å®Ÿè¡Œ ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:919\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    913\u001b[39m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[32m    914\u001b[39m   filtered_flat_args = (\n\u001b[32m    915\u001b[39m       \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.unpack_inputs(\n\u001b[32m    916\u001b[39m           bound_args\n\u001b[32m    917\u001b[39m       )\n\u001b[32m    918\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    920\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[32m    925\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- å­¦ç¿’ã®å®Ÿè¡Œ ---\n",
    "history = model.fit(train_batches, validation_data=val_batches, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1690ad5",
   "metadata": {},
   "source": [
    "###ã€€Result\n",
    "- è‡ªåˆ†ã®PCã®CPU,Core i7ã ã¨90åˆ†ã‹ã‹ã£ãŸ![Result of CPU Learning](images/122101.png)\n",
    "<br>\n",
    "- AIå·¥æˆ¿ã®GPUã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ã†ã¨40ç§’ç¨‹åº¦ã§5epochçµ‚äº†->**135å€!!ã®ã‚¹ãƒ”ãƒ¼ãƒ‰**![Result of GPU Learning](images/122102.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_learning_curve(history):\n",
    "    \"\"\"\n",
    "    å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã€ä½•ã‚¨ãƒãƒƒã‚¯ã§é ­æ‰“ã¡ã«ãªã‚‹ã‹ã‚’è¦–è¦šåŒ–ã™ã‚‹\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history[\"accuracy\"]) + 1)\n",
    "    train_acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "    # é ­æ‰“ã¡ãƒã‚¤ãƒ³ãƒˆã®æ¤œå‡ºï¼ˆæ¤œè¨¼ç²¾åº¦ã®æœ€å¤§å€¤ï¼‰\n",
    "    best_epoch = val_acc.index(max(val_acc)) + 1\n",
    "    best_val_acc = max(val_acc)\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.plot(\n",
    "        epochs,\n",
    "        train_acc,\n",
    "        \"b-\",\n",
    "        label=\"Training Accuracy\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "        markersize=5,\n",
    "    )\n",
    "    ax.plot(\n",
    "        epochs,\n",
    "        val_acc,\n",
    "        \"r-\",\n",
    "        label=\"Validation Accuracy\",\n",
    "        linewidth=2,\n",
    "        marker=\"s\",\n",
    "        markersize=5,\n",
    "    )\n",
    "\n",
    "    # æœ€é«˜ç²¾åº¦ã®ãƒã‚¤ãƒ³ãƒˆã‚’ãƒãƒ¼ã‚¯\n",
    "    ax.axvline(\n",
    "        x=best_epoch,\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f\"Best Epoch: {best_epoch}\",\n",
    "    )\n",
    "    ax.scatter([best_epoch], [best_val_acc], color=\"green\", s=200, zorder=5, marker=\"â˜…\")\n",
    "    ax.annotate(\n",
    "        f\"Best: {best_val_acc:.4f}\\n(Epoch {best_epoch})\",\n",
    "        xy=(best_epoch, best_val_acc),\n",
    "        xytext=(best_epoch + 1, best_val_acc - 0.05),\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"green\"),\n",
    "    )\n",
    "\n",
    "    # é ­æ‰“ã¡é ˜åŸŸã‚’è–„ãå¡—ã‚‹ï¼ˆæœ€é«˜ç²¾åº¦ä»¥é™ï¼‰\n",
    "    if best_epoch < len(epochs):\n",
    "        ax.axvspan(\n",
    "            best_epoch, len(epochs), alpha=0.1, color=\"gray\", label=\"Plateau Region\"\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=14)\n",
    "    ax.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "    ax.set_title(\"Learning Curve: Training vs Validation Accuracy\", fontsize=16)\n",
    "    ax.legend(loc=\"lower right\", fontsize=11)\n",
    "    ax.grid(True, linestyle=\":\", alpha=0.6)\n",
    "    ax.set_xlim(0.5, len(epochs) + 0.5)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "\n",
    "    # Xè»¸ã‚’æ•´æ•°ã‚¨ãƒãƒƒã‚¯ã§è¡¨ç¤º\n",
    "    ax.set_xticks(epochs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"learning_curve.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"å­¦ç¿’æ›²ç·šã‚’ä¿å­˜ã—ã¾ã—ãŸ: learning_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # åˆ†æçµæœã®å‡ºåŠ›\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ã€å­¦ç¿’æ›²ç·šã®åˆ†æã€‘\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"â€¢ æœ€é«˜æ¤œè¨¼ç²¾åº¦: {best_val_acc:.4f} (Epoch {best_epoch})\")\n",
    "    print(f\"â€¢ æœ€çµ‚è¨“ç·´ç²¾åº¦: {train_acc[-1]:.4f}\")\n",
    "    print(f\"â€¢ æœ€çµ‚æ¤œè¨¼ç²¾åº¦: {val_acc[-1]:.4f}\")\n",
    "\n",
    "    # éå­¦ç¿’ã®åˆ¤å®š\n",
    "    gap = train_acc[-1] - val_acc[-1]\n",
    "    if gap > 0.1:\n",
    "        print(f\"â€¢ âš ï¸ éå­¦ç¿’ã®å‚¾å‘ã‚ã‚Šï¼ˆTrain-Valå·®: {gap:.4f}ï¼‰\")\n",
    "    elif val_acc[-1] < best_val_acc - 0.01:\n",
    "        print(f\"â€¢ ğŸŸ¡ Epoch {best_epoch} ä»¥é™ã¯ç²¾åº¦ãŒä½ä¸‹å‚¾å‘\")\n",
    "    else:\n",
    "        print(f\"â€¢ ğŸŸ¢ å®‰å®šã—ãŸå­¦ç¿’ãŒç¶™ç¶š\")\n",
    "\n",
    "    # é ­æ‰“ã¡ã®åˆ¤å®š\n",
    "    if best_epoch <= len(epochs) * 0.5:\n",
    "        print(f\"â€¢ ğŸ“Š Epoch {best_epoch} ã§é ­æ‰“ã¡ â†’ ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æ¸›ã‚‰ã—ã¦ã‚‚è‰¯ã„å¯èƒ½æ€§\")\n",
    "    elif best_epoch >= len(epochs) - 2:\n",
    "        print(f\"â€¢ ğŸ“Š ã¾ã æ”¹å–„ã®ä½™åœ°ã‚ã‚Š â†’ ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™ã¨ç²¾åº¦å‘ä¸Šã®å¯èƒ½æ€§\")\n",
    "\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "plot_learning_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨è©•ä¾¡ ---\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚‚åŒæ§˜ã®å‰å‡¦ç†ï¼ˆãƒªã‚µã‚¤ã‚ºãƒ»æ­£è¦åŒ–ï¼‰ã¨ãƒãƒƒãƒåŒ–ã‚’é©ç”¨\n",
    "test_batches = (\n",
    "    test_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ (æœªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½ç¢ºèª)\n",
    "print(\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™...\")\n",
    "test_loss, test_acc = model.evaluate(test_batches)\n",
    "\n",
    "print(f\"\\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ­£è§£ç‡: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹åã‚’å–å¾—\n",
    "class_names = info.features[\"label\"].names\n",
    "\n",
    "\n",
    "def plot_predictions(dataset, model, num_images=9):\n",
    "    \"\"\"\n",
    "    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ¨è«–ã‚’è¡Œã„ã€ç”»åƒã¨äºˆæ¸¬çµæœã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰1ãƒãƒƒãƒ(32æš)ã ã‘å–ã‚Šå‡ºã™\n",
    "    for images, labels in dataset.take(1):\n",
    "        # æ¨è«–ã®å®Ÿè¡Œ (ç¢ºç‡ãŒå‡ºåŠ›ã•ã‚Œã‚‹)\n",
    "        predictions = model.predict(images)\n",
    "        # æœ€ã‚‚ç¢ºç‡ãŒé«˜ã„ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "        pred_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # æŒ‡å®šæšæ•°åˆ†ã ã‘è¡¨ç¤º\n",
    "        for i in range(min(num_images, len(images))):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i])\n",
    "\n",
    "            # ãƒ©ãƒ™ãƒ«åã®å–å¾—\n",
    "            true_label = class_names[labels[i]]\n",
    "            pred_label = class_names[pred_indices[i]]\n",
    "            confidence = 100 * np.max(predictions[i])\n",
    "\n",
    "            # æ­£è§£ãªã‚‰é’ã€ä¸æ­£è§£ãªã‚‰èµ¤ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º\n",
    "            color = \"blue\" if labels[i] == pred_indices[i] else \"red\"\n",
    "\n",
    "            plt.title(\n",
    "                f\"Pred: {pred_label} ({confidence:.1f}%)\\nTrue: {true_label}\",\n",
    "                color=color,\n",
    "            )\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# å¯è¦–åŒ–ã®å®Ÿè¡Œ\n",
    "print(\"æ¨è«–çµæœã®å¯è¦–åŒ–:\")\n",
    "plot_predictions(test_batches, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba36e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# --- åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---\n",
    "print(\"è©³ç´°ãªã‚¨ãƒ©ãƒ¼åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...\")\n",
    "\n",
    "# å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ã¨æ­£è§£ãƒ©ãƒ™ãƒ«ã®å–å¾—\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "for img_batch, label_batch in test_batches:\n",
    "    all_images.append(img_batch.numpy())\n",
    "    all_labels.append(label_batch.numpy())\n",
    "\n",
    "x_test = np.concatenate(all_images)\n",
    "y_test = np.concatenate(all_labels)\n",
    "\n",
    "# æ¨è«–å®Ÿè¡Œ (ãƒãƒƒãƒå‡¦ç†ã—ã¦ã„ã‚‹ã®ã§é«˜é€Ÿ)\n",
    "predictions = model.predict(x_test, verbose=0)\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "max_probs = np.max(predictions, axis=1)\n",
    "\n",
    "# --- 1. æ··åŒè¡Œåˆ— (Confusion Matrix) ---\n",
    "conf_matrix = confusion_matrix(y_test, pred_labels)\n",
    "\n",
    "# --- 2. ã€Œè‡ªä¿¡æº€ã€…ã«é–“é•ãˆãŸã€ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º ---\n",
    "incorrect_indices = np.where(pred_labels != y_test)[0]\n",
    "# äºˆæ¸¬ç¢ºç‡(è‡ªä¿¡)ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ãƒˆãƒƒãƒ—ã‚’å–å¾—\n",
    "sorted_incorrect_indices = incorrect_indices[\n",
    "    np.argsort(max_probs[incorrect_indices])[::-1]\n",
    "]\n",
    "\n",
    "\n",
    "# --- 3. Grad-CAM (åˆ¤æ–­æ ¹æ‹ ã®å¯è¦–åŒ–) ---\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    \"\"\"\n",
    "    Grad-CAM ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°\n",
    "\n",
    "    \"\"\"\n",
    "    if len(img_array.shape) == 3:\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "\n",
    "def find_target_layer(model):\n",
    "    for layer in reversed(model.layers):\n",
    "        # layer.output_shape ã¯å¤ã„Kerasã®ä»•æ§˜ã§ã™ã€‚ä»£ã‚ã‚Šã« layer.output.shape ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "        try:\n",
    "            if hasattr(layer, \"output\") and len(layer.output.shape) == 4:\n",
    "                return layer.name\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "target_layer_name = find_target_layer(model)\n",
    "\n",
    "# --- å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆ ---\n",
    "plt.figure(figsize=(20, 10))\n",
    "gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 0.5, 0.5])\n",
    "\n",
    "# å·¦å´: æ··åŒè¡Œåˆ—\n",
    "ax_cm = plt.subplot(gs[:, :2])\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    ax=ax_cm,\n",
    ")\n",
    "ax_cm.set_title(\"Confusion Matrix\", fontsize=16)\n",
    "ax_cm.set_ylabel(\"True Label\", fontsize=14)\n",
    "ax_cm.set_xlabel(\"Predicted Label\", fontsize=14)\n",
    "\n",
    "# å³å´: è‡ªä¿¡æº€ã€…ã«é–“é•ãˆãŸç”»åƒ Top 4\n",
    "num_display = 4\n",
    "for i in range(min(num_display, len(sorted_incorrect_indices))):\n",
    "    idx = sorted_incorrect_indices[i]\n",
    "    img = x_test[idx]\n",
    "    true_lb = class_names[y_test[idx]]\n",
    "    pred_lb = class_names[pred_labels[idx]]\n",
    "    conf = max_probs[idx]\n",
    "\n",
    "    # Grad-CAM ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
    "    heatmap = make_gradcam_heatmap(\n",
    "        img, model, target_layer_name, pred_index=pred_labels[idx]\n",
    "    )\n",
    "\n",
    "    # ãƒªã‚µã‚¤ã‚ºã¨é‡ã­åˆã‚ã›è¡¨ç¤º\n",
    "    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], (IMG_SIZE, IMG_SIZE))\n",
    "    heatmap_resized = tf.squeeze(heatmap_resized).numpy()\n",
    "\n",
    "    # ã‚°ãƒªãƒƒãƒ‰é…ç½® (å³åŠåˆ†ã®é ˜åŸŸã‚’ä½¿ã†)\n",
    "    row = i // 2\n",
    "    col = 2 + (i % 2)\n",
    "    ax_img = plt.subplot(gs[row, col])\n",
    "\n",
    "    ax_img.imshow(img)\n",
    "    ax_img.imshow(heatmap_resized, alpha=0.5, cmap=\"jet\")\n",
    "\n",
    "    # æ ç·šã®è‰² (èµ¤: é–“é•ã„)\n",
    "    for spine in ax_img.spines.values():\n",
    "        spine.set_edgecolor(\"red\")\n",
    "        spine.set_linewidth(3)\n",
    "\n",
    "    ax_img.set_title(\n",
    "        f\"True: {true_lb}\\nPred: {pred_lb}\\nConf: {conf:.1%}\",\n",
    "        fontsize=12,\n",
    "        color=\"red\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax_img.set_xticks([])\n",
    "    ax_img.set_yticks([])\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Error Analysis & Grad-CAM (Target Layer: {target_layer_name})\", fontsize=20\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# è€ƒå¯Ÿã‚³ãƒ¡ãƒ³ãƒˆã®ä¾‹ç¤º\n",
    "print(\"\\n=== è€ƒå¯Ÿã‚³ãƒ¡ãƒ³ãƒˆã®ä¾‹ ===\")\n",
    "print(\n",
    "    \"Grad-CAMã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆèµ¤è‰²éƒ¨åˆ†ï¼‰ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒç”»åƒã®ã©ã“ã‚’è¦‹ã¦åˆ¤æ–­ã—ãŸã‹ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚\"\n",
    ")\n",
    "print(\n",
    "    \"ã“ã‚Œã‚’ç”¨ã„ã¦ã€ã€å½¢ã§ã¯ãªãè‰²ã ã‘ã§åˆ¤æ–­ã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã€ãªã©ã®èª¤ç­”åŸå› ã‚’åˆ†æã§ãã¾ã™ã€‚\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_color_histograms(dataset, class_names, target_classes):\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã—ãŸ2ã¤ã®ã‚¯ãƒ©ã‚¹ã®ã€Œè‰²ã®åˆ†å¸ƒã€ã‚’æ¯”è¼ƒã™ã‚‹\n",
    "    target_classes: ['SeaLake', 'Pasture'] ã®ã‚ˆã†ã«æŒ‡å®š\n",
    "    \"\"\"\n",
    "    colors = [\"Red\", \"Green\", \"Blue\"]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚’å°‘ã—é›†ã‚ã‚‹\n",
    "    target_images = {name: [] for name in target_classes}\n",
    "\n",
    "    for img, label in dataset.take(200):  # 200æšãã‚‰ã„ãƒã‚§ãƒƒã‚¯\n",
    "        name = class_names[label.numpy()]\n",
    "        if name in target_classes:\n",
    "            target_images[name].append(img.numpy())\n",
    "\n",
    "    # å„ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨ã«ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’æç”»\n",
    "    for i, color in enumerate(colors):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        for name in target_classes:\n",
    "            if len(target_images[name]) == 0:\n",
    "                continue\n",
    "\n",
    "            # å…¨ç”»åƒã®ãƒ”ã‚¯ã‚»ãƒ«å€¤ã‚’ã¾ã¨ã‚ã‚‹\n",
    "            pixels = np.concatenate(target_images[name])\n",
    "            pixel_values = pixels[:, :, i].flatten()  # 1æ¬¡å…ƒã«ä¼¸ã°ã™\n",
    "\n",
    "            plt.hist(pixel_values, bins=50, alpha=0.5, label=name, density=True)\n",
    "\n",
    "        plt.title(f\"{color} Channel Distribution\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Pixel Value (0-1)\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "plot_color_histograms(test_ds, info.features[\"label\"].names, [\"SeaLake\", \"Pasture\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f21044a",
   "metadata": {},
   "source": [
    "### RGB 3Dåˆ†å¸ƒã«ã‚ˆã‚‹èª¤åˆ†é¡åŸå› ã®åˆ†æ\n",
    "\n",
    "ResNetã®èª¤åˆ†é¡ãŒRGBï¼ˆè‰²ï¼‰ã«èµ·å› ã™ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã®åˆ†æã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "#### åˆ†ææ‰‹æ³•\n",
    "1. **3Dæ•£å¸ƒå›³**: å„ç”»åƒã®RGBå¹³å‡å€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "2. **å®šé‡åˆ†æ**: èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ãŒã€Œå…ƒã‚¯ãƒ©ã‚¹ã€ã¨ã€Œæ··åŒå…ˆã‚¯ãƒ©ã‚¹ã€ã©ã¡ã‚‰ã«è¿‘ã„ã‹è¨ˆç®—\n",
    "3. **çµ±è¨ˆçš„æ¤œå®š**: æ­£è§£/èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«é–“ã§RGBç‰¹æ€§ã«æœ‰æ„å·®ãŒã‚ã‚‹ã‹æ¤œè¨¼\n",
    "4. **RGBåˆ†æ•£åˆ†æ**: ç”»åƒå†…ã®è‰²ã®ã°ã‚‰ã¤ããŒèª¤åˆ†é¡ã«é–¢ä¿‚ã™ã‚‹ã‹èª¿æŸ»\n",
    "\n",
    "#### è€ƒå¯Ÿã®è¦³ç‚¹\n",
    "\n",
    "| è¦³å¯Ÿçµæœ | è§£é‡ˆ |\n",
    "|---------|------|\n",
    "| èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ãŒæ··åŒå…ˆã‚¯ãƒ©ã‚¹ã®é ˜åŸŸã«ã‚ã‚‹ | â†’ RGBã®é¡ä¼¼æ€§ãŒåŸå›  |\n",
    "| èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ãŒå…ƒã‚¯ãƒ©ã‚¹å†…ã«æ•£åœ¨ã—ã¦ã„ã‚‹ | â†’ RGBä»¥å¤–ã®è¦å› ï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£ã€ã‚¨ãƒƒã‚¸ã€å½¢çŠ¶ï¼‰ãŒåŸå›  |\n",
    "| èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã«ç‰¹å®šã®RGBå‚¾å‘ãŒãªã„ | â†’ RGBã¯åˆ†é¡ã«å¯„ä¸ã—ã¦ã„ãªã„å¯èƒ½æ€§ |\n",
    "\n",
    "#### RGBå¹³å‡ã®é™ç•Œ\n",
    "- ç”»åƒå…¨ä½“ã®å¹³å‡å€¤ã¯**å±€æ‰€çš„ãªç‰¹å¾´ã‚’æ¶ˆã—ã¦ã—ã¾ã†**\n",
    "- åŒã˜RGBå¹³å‡ã§ã‚‚ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£ã‚„ç©ºé–“ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ç•°ãªã‚Šã†ã‚‹\n",
    "- è¡›æ˜Ÿç”»åƒã§ã¯ã€ç‰©ä½“ã®**é…ç½®**ã‚„**ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³**ãŒé‡è¦ãªã“ã¨ãŒå¤šã„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def plot_rgb_3d_interactive(x_test, y_test, pred_labels, class_names):\n",
    "    \"\"\"\n",
    "    RGBå¹³å‡å€¤ã‚’3Dç©ºé–“ã«ãƒ—ãƒ­ãƒƒãƒˆã—ã€æ­£è§£/èª¤åˆ†é¡ã‚’å¯è¦–åŒ–ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_test : æ­£è¦åŒ–æ¸ˆã¿ãƒ†ã‚¹ãƒˆç”»åƒ (0-1)\n",
    "    y_test : æ­£è§£ãƒ©ãƒ™ãƒ«\n",
    "    pred_labels : äºˆæ¸¬ãƒ©ãƒ™ãƒ«\n",
    "    class_names : ã‚¯ãƒ©ã‚¹åãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. å„ç”»åƒã®RGBå¹³å‡å€¤ã‚’è¨ˆç®—\n",
    "    # ==========================================\n",
    "    print(\"å„ç”»åƒã®RGBå¹³å‡å€¤ã‚’è¨ˆç®—ä¸­...\")\n",
    "\n",
    "    # ç”»åƒã”ã¨ã®RGBå¹³å‡ (0-255ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™)\n",
    "    rgb_means = np.mean(x_test, axis=(1, 2)) * 255  # shape: (N, 3)\n",
    "\n",
    "    # æ­£è§£/èª¤åˆ†é¡ã®ãƒã‚¹ã‚¯\n",
    "    correct_mask = pred_labels == y_test\n",
    "    incorrect_mask = ~correct_mask\n",
    "\n",
    "    print(f\"æ­£è§£æ•°: {np.sum(correct_mask)}, èª¤åˆ†é¡æ•°: {np.sum(incorrect_mask)}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. Plotlyã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–3Dãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    # ==========================================\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ— (10ã‚¯ãƒ©ã‚¹ç”¨)\n",
    "    colors = [\n",
    "        \"#1f77b4\",\n",
    "        \"#ff7f0e\",\n",
    "        \"#2ca02c\",\n",
    "        \"#d62728\",\n",
    "        \"#9467bd\",\n",
    "        \"#8c564b\",\n",
    "        \"#e377c2\",\n",
    "        \"#7f7f7f\",\n",
    "        \"#bcbd22\",\n",
    "        \"#17becf\",\n",
    "    ]\n",
    "\n",
    "    # --- æ­£è§£ã‚µãƒ³ãƒ—ãƒ« (è–„ãè¡¨ç¤º) ---\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = (y_test == i) & correct_mask\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "\n",
    "        data = rgb_means[mask]\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=data[:, 0],\n",
    "                y=data[:, 1],\n",
    "                z=data[:, 2],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=3, color=colors[i], opacity=0.3),\n",
    "                name=f\"{class_name} (æ­£è§£)\",\n",
    "                legendgroup=class_name,\n",
    "                hovertemplate=f\"<b>{class_name}</b><br>R: %{{x:.1f}}<br>G: %{{y:.1f}}<br>B: %{{z:.1f}}<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # --- èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ« (å¤§ããç›®ç«‹ãŸã›ã‚‹) ---\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # ã“ã®ã‚¯ãƒ©ã‚¹ã«å±ã™ã‚‹ç”»åƒã§èª¤åˆ†é¡ã•ã‚ŒãŸã‚‚ã®\n",
    "        mask = (y_test == i) & incorrect_mask\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "\n",
    "        data = rgb_means[mask]\n",
    "        pred_class_indices = pred_labels[mask]\n",
    "\n",
    "        # ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã«äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã‚‚è¡¨ç¤º\n",
    "        hover_texts = [\n",
    "            f\"<b>True: {class_name}</b><br>Pred: {class_names[p]}<br>R: {r:.1f}, G: {g:.1f}, B: {b:.1f}\"\n",
    "            for (r, g, b), p in zip(data, pred_class_indices)\n",
    "        ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=data[:, 0],\n",
    "                y=data[:, 1],\n",
    "                z=data[:, 2],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=colors[i],\n",
    "                    opacity=1.0,\n",
    "                    symbol=\"x\",  # Ã—ãƒãƒ¼ã‚«ãƒ¼ã§èª¤åˆ†é¡ã‚’å¼·èª¿\n",
    "                    line=dict(width=2, color=\"black\"),\n",
    "                ),\n",
    "                name=f\"{class_name} (èª¤åˆ†é¡)\",\n",
    "                legendgroup=class_name,\n",
    "                hovertemplate=\"%{text}<extra></extra>\",\n",
    "                text=hover_texts,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # --- å„ã‚¯ãƒ©ã‚¹ã®é‡å¿ƒã‚’è¡¨ç¤º ---\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = y_test == i\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "\n",
    "        center = np.mean(rgb_means[mask], axis=0)\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[center[0]],\n",
    "                y=[center[1]],\n",
    "                z=[center[2]],\n",
    "                mode=\"markers+text\",\n",
    "                marker=dict(\n",
    "                    size=15,\n",
    "                    color=colors[i],\n",
    "                    symbol=\"diamond\",\n",
    "                    line=dict(width=3, color=\"white\"),\n",
    "                ),\n",
    "                text=[class_name],\n",
    "                textposition=\"top center\",\n",
    "                textfont=dict(size=12, color=\"black\"),\n",
    "                name=f\"{class_name} é‡å¿ƒ\",\n",
    "                legendgroup=class_name,\n",
    "                showlegend=False,\n",
    "                hovertemplate=f\"<b>{class_name} é‡å¿ƒ</b><br>R: {center[0]:.1f}<br>G: {center[1]:.1f}<br>B: {center[2]:.1f}<extra></extra>\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"RGB Distribution - æ­£è§£ vs èª¤åˆ†é¡<br><sub>Ã—ãƒãƒ¼ã‚«ãƒ¼ = èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«</sub>\",\n",
    "            font=dict(size=20),\n",
    "        ),\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Red Mean\",\n",
    "            yaxis_title=\"Green Mean\",\n",
    "            zaxis_title=\"Blue Mean\",\n",
    "            xaxis=dict(range=[0, 255]),\n",
    "            yaxis=dict(range=[0, 255]),\n",
    "            zaxis=dict(range=[0, 255]),\n",
    "            aspectmode=\"cube\",\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=1.02, font=dict(size=10)),\n",
    "        margin=dict(r=200),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. èª¤åˆ†é¡ã®çµ±è¨ˆæƒ…å ±\n",
    "    # ==========================================\n",
    "    print(\"\\n=== èª¤åˆ†é¡ã®RGBåˆ†æ ===\")\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        correct = rgb_means[(y_test == i) & correct_mask]\n",
    "        incorrect = rgb_means[(y_test == i) & incorrect_mask]\n",
    "\n",
    "        if len(incorrect) > 0 and len(correct) > 0:\n",
    "            correct_mean = np.mean(correct, axis=0)\n",
    "            incorrect_mean = np.mean(incorrect, axis=0)\n",
    "            diff = incorrect_mean - correct_mean\n",
    "\n",
    "            print(f\"\\nã€{class_name}ã€‘ èª¤åˆ†é¡: {len(incorrect)}ä»¶\")\n",
    "            print(\n",
    "                f\"  æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡RGB: R={correct_mean[0]:.1f}, G={correct_mean[1]:.1f}, B={correct_mean[2]:.1f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡RGB: R={incorrect_mean[0]:.1f}, G={incorrect_mean[1]:.1f}, B={incorrect_mean[2]:.1f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  å·®åˆ† (èª¤åˆ†é¡-æ­£è§£): Î”R={diff[0]:+.1f}, Î”G={diff[1]:+.1f}, Î”B={diff[2]:+.1f}\"\n",
    "            )\n",
    "\n",
    "    return rgb_means, correct_mask, incorrect_mask\n",
    "\n",
    "\n",
    "# å®Ÿè¡Œ (å‰ã®ã‚»ãƒ«ã§ x_test, y_test, pred_labels ãŒå®šç¾©æ¸ˆã¿ã®å‰æ)\n",
    "print(\"3D RGBåˆ†å¸ƒã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™...\")\n",
    "rgb_means, correct_mask, incorrect_mask = plot_rgb_3d_interactive(\n",
    "    x_test, y_test, pred_labels, class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d04483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_pairs_rgb(x_test, y_test, pred_labels, class_names, top_n=5):\n",
    "    \"\"\"\n",
    "    èª¤åˆ†é¡ãŒå¤šã„ã‚¯ãƒ©ã‚¹ãƒšã‚¢ã®RGBåˆ†å¸ƒã‚’è©³ç´°æ¯”è¼ƒã™ã‚‹\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    from scipy import stats\n",
    "\n",
    "    # èª¤åˆ†é¡ãƒšã‚¢ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    incorrect_mask = pred_labels != y_test\n",
    "    pairs = [(y_test[i], pred_labels[i]) for i in np.where(incorrect_mask)[0]]\n",
    "    pair_counts = Counter(pairs)\n",
    "\n",
    "    print(\"=== èª¤åˆ†é¡ãƒšã‚¢ Top {} ===\".format(top_n))\n",
    "    for (true_idx, pred_idx), count in pair_counts.most_common(top_n):\n",
    "        print(f\"  {class_names[true_idx]} â†’ {class_names[pred_idx]}: {count}ä»¶\")\n",
    "\n",
    "    if len(pair_counts) == 0:\n",
    "        print(\"èª¤åˆ†é¡ãŒã‚ã‚Šã¾ã›ã‚“ï¼\")\n",
    "        return\n",
    "\n",
    "    top_pairs = pair_counts.most_common(min(top_n, 4))\n",
    "\n",
    "    # RGBå¹³å‡å€¤\n",
    "    rgb_means = np.mean(x_test, axis=(1, 2)) * 255\n",
    "\n",
    "    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ (2x2)\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[\n",
    "            [{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}],\n",
    "            [{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}],\n",
    "        ],\n",
    "        subplot_titles=[\n",
    "            f\"{class_names[t]} â†” {class_names[p]} ({c}ä»¶)\" for (t, p), c in top_pairs\n",
    "        ]\n",
    "        + [\"\"] * (4 - len(top_pairs)),\n",
    "    )\n",
    "\n",
    "    colors_correct = [\"#2196F3\", \"#4CAF50\"]\n",
    "    colors_incorrect = [\"#F44336\", \"#FF9800\"]\n",
    "\n",
    "    # ==========================================\n",
    "    # å®šé‡åˆ†æã®æº–å‚™\n",
    "    # ==========================================\n",
    "    analysis_results = []\n",
    "\n",
    "    for idx, ((true_idx, pred_idx), count) in enumerate(top_pairs):\n",
    "        row = idx // 2 + 1\n",
    "        col = idx % 2 + 1\n",
    "\n",
    "        true_name = class_names[true_idx]\n",
    "        pred_name = class_names[pred_idx]\n",
    "\n",
    "        true_class_mask = y_test == true_idx\n",
    "        true_correct = true_class_mask & (pred_labels == y_test)\n",
    "        true_wrong = true_class_mask & (pred_labels == pred_idx)\n",
    "\n",
    "        pred_class_mask = y_test == pred_idx\n",
    "        pred_correct = pred_class_mask & (pred_labels == y_test)\n",
    "\n",
    "        # --- å®šé‡åˆ†æ ---\n",
    "        correct_data = rgb_means[true_correct]\n",
    "        wrong_data = rgb_means[true_wrong]\n",
    "        pred_class_data = rgb_means[pred_correct]\n",
    "\n",
    "        if len(wrong_data) > 0 and len(correct_data) > 0 and len(pred_class_data) > 0:\n",
    "            # å„ã‚¯ãƒ©ã‚¹ã®é‡å¿ƒ\n",
    "            correct_center = np.mean(correct_data, axis=0)\n",
    "            wrong_center = np.mean(wrong_data, axis=0)\n",
    "            pred_center = np.mean(pred_class_data, axis=0)\n",
    "\n",
    "            # èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ãŒã€Œå…ƒã‚¯ãƒ©ã‚¹é‡å¿ƒã€ã¨ã€Œæ··åŒå…ˆã‚¯ãƒ©ã‚¹é‡å¿ƒã€ã®ã©ã¡ã‚‰ã«è¿‘ã„ã‹\n",
    "            dist_to_own = np.linalg.norm(wrong_center - correct_center)\n",
    "            dist_to_pred = np.linalg.norm(wrong_center - pred_center)\n",
    "\n",
    "            # èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®åˆ†æ•£ï¼ˆæ•£ã‚‰ã°ã‚Šå…·åˆï¼‰\n",
    "            wrong_variance = np.mean(np.var(wrong_data, axis=0))\n",
    "            correct_variance = np.mean(np.var(correct_data, axis=0))\n",
    "\n",
    "            # ã‚¯ãƒ©ã‚¹é–“ã®é‡è¤‡åº¦ï¼ˆé‡å¿ƒé–“è·é›¢ / åˆ†æ•£ï¼‰\n",
    "            class_separation = np.linalg.norm(correct_center - pred_center)\n",
    "\n",
    "            analysis_results.append(\n",
    "                {\n",
    "                    \"pair\": f\"{true_name} â†’ {pred_name}\",\n",
    "                    \"count\": count,\n",
    "                    \"dist_to_own\": dist_to_own,\n",
    "                    \"dist_to_pred\": dist_to_pred,\n",
    "                    \"closer_to\": \"pred_class\"\n",
    "                    if dist_to_pred < dist_to_own\n",
    "                    else \"own_class\",\n",
    "                    \"class_separation\": class_separation,\n",
    "                    \"wrong_variance\": wrong_variance,\n",
    "                    \"correct_variance\": correct_variance,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "        if len(correct_data) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=correct_data[:, 0],\n",
    "                    y=correct_data[:, 1],\n",
    "                    z=correct_data[:, 2],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=4, color=colors_correct[0], opacity=0.5),\n",
    "                    name=f\"{true_name} (æ­£è§£)\",\n",
    "                    showlegend=(idx == 0),\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "        if len(wrong_data) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=wrong_data[:, 0],\n",
    "                    y=wrong_data[:, 1],\n",
    "                    z=wrong_data[:, 2],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        size=8, color=colors_incorrect[0], opacity=1.0, symbol=\"x\"\n",
    "                    ),\n",
    "                    name=f\"{true_name}â†’{pred_name} (èª¤åˆ†é¡)\",\n",
    "                    showlegend=(idx == 0),\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "        if len(pred_class_data) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=pred_class_data[:, 0],\n",
    "                    y=pred_class_data[:, 1],\n",
    "                    z=pred_class_data[:, 2],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=4, color=colors_correct[1], opacity=0.5),\n",
    "                    name=f\"{pred_name} (æ­£è§£)\",\n",
    "                    showlegend=(idx == 0),\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"èª¤åˆ†é¡ãƒšã‚¢åˆ¥ RGBåˆ†å¸ƒæ¯”è¼ƒ\",\n",
    "        height=900,\n",
    "        width=1100,\n",
    "    )\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        fig.update_scenes(\n",
    "            dict(xaxis_title=\"R\", yaxis_title=\"G\", zaxis_title=\"B\", aspectmode=\"cube\"),\n",
    "            row=(i - 1) // 2 + 1,\n",
    "            col=(i - 1) % 2 + 1,\n",
    "        )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # ==========================================\n",
    "    # å®šé‡çš„ãªè€ƒå¯Ÿã®å‡ºåŠ›\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ã€å®šé‡åˆ†æã«ã‚ˆã‚‹è€ƒå¯Ÿã€‘\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for result in analysis_results:\n",
    "        print(f\"\\nâ–  {result['pair']} ({result['count']}ä»¶)\")\n",
    "        print(\n",
    "            f\"  â”œâ”€ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ« â†’ å…ƒã‚¯ãƒ©ã‚¹é‡å¿ƒã¾ã§ã®è·é›¢: {result['dist_to_own']:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  â”œâ”€ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ« â†’ æ··åŒå…ˆã‚¯ãƒ©ã‚¹é‡å¿ƒã¾ã§ã®è·é›¢: {result['dist_to_pred']:.2f}\"\n",
    "        )\n",
    "        print(f\"  â”œâ”€ 2ã‚¯ãƒ©ã‚¹é–“ã®é‡å¿ƒè·é›¢ (åˆ†é›¢åº¦): {result['class_separation']:.2f}\")\n",
    "        print(\n",
    "            f\"  â””â”€ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®åˆ†æ•£: {result['wrong_variance']:.2f} (æ­£è§£: {result['correct_variance']:.2f})\"\n",
    "        )\n",
    "\n",
    "        # è€ƒå¯Ÿã®è‡ªå‹•åˆ¤å®š\n",
    "        if result[\"dist_to_pred\"] < result[\"dist_to_own\"]:\n",
    "            print(\n",
    "                f\"  â†’ ğŸ”´ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯æ··åŒå…ˆã‚¯ãƒ©ã‚¹ã«è¿‘ã„ â†’ RGBãŒèª¤åˆ†é¡ã®åŸå› ã®å¯èƒ½æ€§\"\n",
    "            )\n",
    "        elif result[\"dist_to_own\"] < result[\"class_separation\"] * 0.3:\n",
    "            print(\n",
    "                f\"  â†’ ğŸŸ¢ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯å…ƒã‚¯ãƒ©ã‚¹å†…ã«ç•™ã¾ã£ã¦ã„ã‚‹ â†’ RGBä»¥å¤–ã®è¦å› ãŒåŸå› \"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  â†’ ğŸŸ¡ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯ä¸¡ã‚¯ãƒ©ã‚¹ã®ä¸­é–“é ˜åŸŸ â†’ ã‚¯ãƒ©ã‚¹å¢ƒç•ŒãŒæ›–æ˜§\")\n",
    "\n",
    "    # å…¨ä½“çš„ãªè€ƒå¯Ÿ\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ã€RGBç©ºé–“ã§ã®åˆ†æã¾ã¨ã‚ã€‘\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    rgb_caused = sum(1 for r in analysis_results if r[\"closer_to\"] == \"pred_class\")\n",
    "    total = len(analysis_results)\n",
    "\n",
    "    if total > 0:\n",
    "        rgb_ratio = rgb_caused / total\n",
    "        print(\n",
    "            f\"\\nâ€¢ èª¤åˆ†é¡ãŒRGBã«èµ·å› ã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ãƒšã‚¢: {rgb_caused}/{total} ({rgb_ratio * 100:.0f}%)\"\n",
    "        )\n",
    "\n",
    "        if rgb_ratio > 0.5:\n",
    "            print(\n",
    "                \"\\nğŸ“Š çµè«–: èª¤åˆ†é¡ã®å¤šãã¯RGBï¼ˆè‰²ï¼‰ã®é¡ä¼¼æ€§ã«èµ·å› ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„\"\n",
    "            )\n",
    "            print(\"   â†’ è‰²ãŒä¼¼ã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹é–“ã§ã®èª¤åˆ†é¡ãŒå¤šã„\")\n",
    "            print(\"   â†’ å¯¾ç­–: è‰²ä»¥å¤–ã®ç‰¹å¾´ï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£ã€ã‚¨ãƒƒã‚¸ï¼‰ã‚’å¼·èª¿ã™ã‚‹å‰å‡¦ç†ã‚„ã€\")\n",
    "            print(\"           Data Augmentationï¼ˆè‰²å¤‰æ›ï¼‰ãŒæœ‰åŠ¹ã‹ã‚‚ã—ã‚Œãªã„\")\n",
    "        else:\n",
    "            print(\"\\nğŸ“Š çµè«–: èª¤åˆ†é¡ã¯RGBã®é¡ä¼¼æ€§ã ã‘ã§ã¯èª¬æ˜ã§ããªã„\")\n",
    "            print(\"   â†’ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯å…ƒã®ã‚¯ãƒ©ã‚¹ã¨åŒã˜RGBé ˜åŸŸã«å­˜åœ¨ã—ã¦ã„ã‚‹\")\n",
    "            print(\"   â†’ RGBå¹³å‡å€¤ã ã‘ã§ã¯æ‰ãˆã‚‰ã‚Œãªã„ç‰¹å¾´ãŒåˆ†é¡ã«é‡è¦\")\n",
    "            print(\"   â†’ è€ƒãˆã‚‰ã‚Œã‚‹è¦å› :\")\n",
    "            print(\"     â€¢ ãƒ†ã‚¯ã‚¹ãƒãƒ£ï¼ˆç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰\")\n",
    "            print(\"     â€¢ ã‚¨ãƒƒã‚¸ãƒ»è¼ªéƒ­ã®å½¢çŠ¶\")\n",
    "            print(\"     â€¢ ç©ºé–“çš„ãªè‰²ã®é…ç½®ï¼ˆç”»åƒã®ä¸Šä¸‹å·¦å³ã§ã®è‰²ã®é•ã„ï¼‰\")\n",
    "            print(\"     â€¢ å±€æ‰€çš„ãªç‰¹å¾´ï¼ˆRGBå¹³å‡ã§ã¯æ¶ˆãˆã¦ã—ã¾ã†ï¼‰\")\n",
    "\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "plot_confusion_pairs_rgb(x_test, y_test, pred_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671446eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rgb_distribution_detail(x_test, y_test, pred_labels, class_names):\n",
    "    \"\"\"\n",
    "    ã‚ˆã‚Šè©³ç´°ãªRGBåˆ†æï¼šåˆ†æ•£ã€ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã€ç©ºé–“çš„åˆ†å¸ƒ\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    incorrect_mask = pred_labels != y_test\n",
    "    correct_mask = ~incorrect_mask\n",
    "\n",
    "    # RGBå¹³å‡ã¨åˆ†æ•£\n",
    "    rgb_means = np.mean(x_test, axis=(1, 2)) * 255\n",
    "    rgb_stds = np.std(x_test, axis=(1, 2)) * 255  # ç”»åƒå†…ã®RGBåˆ†æ•£\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. èª¤åˆ†é¡vsæ­£è§£ã® RGBåˆ†æ•£æ¯”è¼ƒ\n",
    "    # ==========================================\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[\n",
    "            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "        ],\n",
    "        subplot_titles=[\n",
    "            \"Red: å¹³å‡ vs åˆ†æ•£\",\n",
    "            \"Green: å¹³å‡ vs åˆ†æ•£\",\n",
    "            \"Blue: å¹³å‡ vs åˆ†æ•£\",\n",
    "            \"RGBåˆ†æ•£ã®åˆè¨ˆ (æ­£è§£ vs èª¤åˆ†é¡)\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    channel_names = [\"Red\", \"Green\", \"Blue\"]\n",
    "\n",
    "    for ch, name in enumerate(channel_names):\n",
    "        row = (ch // 2) + 1\n",
    "        col = (ch % 2) + 1\n",
    "\n",
    "        # æ­£è§£ã‚µãƒ³ãƒ—ãƒ«\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rgb_means[correct_mask, ch],\n",
    "                y=rgb_stds[correct_mask, ch],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=3, color=\"blue\", opacity=0.3),\n",
    "                name=\"æ­£è§£\",\n",
    "                showlegend=(ch == 0),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        # èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rgb_means[incorrect_mask, ch],\n",
    "                y=rgb_stds[incorrect_mask, ch],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=6, color=\"red\", opacity=0.7),\n",
    "                name=\"èª¤åˆ†é¡\",\n",
    "                showlegend=(ch == 0),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "    # RGBåˆ†æ•£ã®åˆè¨ˆ\n",
    "    total_std_correct = np.sum(rgb_stds[correct_mask], axis=1)\n",
    "    total_std_incorrect = np.sum(rgb_stds[incorrect_mask], axis=1)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=total_std_correct,\n",
    "            name=\"æ­£è§£\",\n",
    "            opacity=0.6,\n",
    "            marker_color=\"blue\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=total_std_incorrect,\n",
    "            name=\"èª¤åˆ†é¡\",\n",
    "            opacity=0.6,\n",
    "            marker_color=\"red\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"RGBå¹³å‡ vs RGBåˆ†æ•£ï¼ˆç”»åƒå†…ã®ã°ã‚‰ã¤ãï¼‰\",\n",
    "        height=700,\n",
    "        width=900,\n",
    "        barmode=\"overlay\",\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. çµ±è¨ˆçš„æ¤œå®š\n",
    "    # ==========================================\n",
    "    from scipy import stats\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ã€çµ±è¨ˆçš„åˆ†æã€‘èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å¾´\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # RGBå¹³å‡ã®æ¯”è¼ƒ\n",
    "    print(\"\\nâ–  RGBå¹³å‡å€¤ã®æ¯”è¼ƒ (tæ¤œå®š)\")\n",
    "    for ch, name in enumerate(channel_names):\n",
    "        correct_vals = rgb_means[correct_mask, ch]\n",
    "        incorrect_vals = rgb_means[incorrect_mask, ch]\n",
    "        t_stat, p_val = stats.ttest_ind(correct_vals, incorrect_vals)\n",
    "        sig = \"æœ‰æ„å·®ã‚ã‚Š âœ“\" if p_val < 0.05 else \"æœ‰æ„å·®ãªã—\"\n",
    "        print(\n",
    "            f\"  {name}: æ­£è§£={np.mean(correct_vals):.1f}Â±{np.std(correct_vals):.1f}, \"\n",
    "            f\"èª¤åˆ†é¡={np.mean(incorrect_vals):.1f}Â±{np.std(incorrect_vals):.1f} (p={p_val:.4f}) {sig}\"\n",
    "        )\n",
    "\n",
    "    # RGBåˆ†æ•£ã®æ¯”è¼ƒ\n",
    "    print(\"\\nâ–  RGBåˆ†æ•£ï¼ˆç”»åƒå†…ã°ã‚‰ã¤ãï¼‰ã®æ¯”è¼ƒ\")\n",
    "    for ch, name in enumerate(channel_names):\n",
    "        correct_vals = rgb_stds[correct_mask, ch]\n",
    "        incorrect_vals = rgb_stds[incorrect_mask, ch]\n",
    "        t_stat, p_val = stats.ttest_ind(correct_vals, incorrect_vals)\n",
    "        sig = \"æœ‰æ„å·®ã‚ã‚Š âœ“\" if p_val < 0.05 else \"æœ‰æ„å·®ãªã—\"\n",
    "        diff = np.mean(incorrect_vals) - np.mean(correct_vals)\n",
    "        direction = \"é«˜ã„â†‘\" if diff > 0 else \"ä½ã„â†“\"\n",
    "        print(\n",
    "            f\"  {name}: èª¤åˆ†é¡ã¯æ­£è§£ã‚ˆã‚Šåˆ†æ•£ãŒ{direction} (å·®={diff:+.2f}, p={p_val:.4f}) {sig}\"\n",
    "        )\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. ç©ºé–“çš„ãªRGBåˆ†å¸ƒï¼ˆä¸Šä¸‹å·¦å³ï¼‰\n",
    "    # ==========================================\n",
    "    print(\"\\nâ–  ç©ºé–“çš„ãªRGBåˆ†å¸ƒã®åˆ†æ\")\n",
    "\n",
    "    # ç”»åƒã‚’ä¸ŠåŠåˆ†ã¨ä¸‹åŠåˆ†ã«åˆ†ã‘ã¦æ¯”è¼ƒ\n",
    "    h = x_test.shape[1] // 2\n",
    "    top_mean = np.mean(x_test[:, :h, :, :], axis=(1, 2)) * 255\n",
    "    bottom_mean = np.mean(x_test[:, h:, :, :], axis=(1, 2)) * 255\n",
    "    vertical_diff = top_mean - bottom_mean  # ä¸Šä¸‹ã®è‰²ã®é•ã„\n",
    "\n",
    "    # èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯ä¸Šä¸‹ã®è‰²å·®ãŒå¤§ãã„/å°ã•ã„ï¼Ÿ\n",
    "    correct_v_diff = np.mean(np.abs(vertical_diff[correct_mask]), axis=1)\n",
    "    incorrect_v_diff = np.mean(np.abs(vertical_diff[incorrect_mask]), axis=1)\n",
    "\n",
    "    t_stat, p_val = stats.ttest_ind(correct_v_diff, incorrect_v_diff)\n",
    "    print(\n",
    "        f\"  ä¸Šä¸‹ã®è‰²å·®: æ­£è§£={np.mean(correct_v_diff):.2f}, èª¤åˆ†é¡={np.mean(incorrect_v_diff):.2f} (p={p_val:.4f})\"\n",
    "    )\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. è€ƒå¯Ÿã®ã¾ã¨ã‚\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ã€è€ƒå¯Ÿã€‘\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # RGBå¹³å‡ã«æœ‰æ„å·®ãŒã‚ã‚‹ã‹\n",
    "    rgb_mean_significant = False\n",
    "    for ch in range(3):\n",
    "        _, p_val = stats.ttest_ind(\n",
    "            rgb_means[correct_mask, ch], rgb_means[incorrect_mask, ch]\n",
    "        )\n",
    "        if p_val < 0.05:\n",
    "            rgb_mean_significant = True\n",
    "            break\n",
    "\n",
    "    # RGBåˆ†æ•£ã«æœ‰æ„å·®ãŒã‚ã‚‹ã‹\n",
    "    rgb_std_significant = False\n",
    "    for ch in range(3):\n",
    "        _, p_val = stats.ttest_ind(\n",
    "            rgb_stds[correct_mask, ch], rgb_stds[incorrect_mask, ch]\n",
    "        )\n",
    "        if p_val < 0.05:\n",
    "            rgb_std_significant = True\n",
    "            break\n",
    "\n",
    "    if not rgb_mean_significant and not rgb_std_significant:\n",
    "        print(\"\\nğŸŸ¢ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¨æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ã®RGBç‰¹æ€§ã«çµ±è¨ˆçš„ãªæœ‰æ„å·®ãŒãªã„\")\n",
    "        print(\"   â†’ RGBã®å¹³å‡å€¤ãƒ»åˆ†æ•£ã ã‘ã§ã¯èª¤åˆ†é¡ã‚’èª¬æ˜ã§ããªã„\")\n",
    "        print(\n",
    "            \"   â†’ ãƒ¢ãƒ‡ãƒ«ã¯RGBä»¥å¤–ã®ç‰¹å¾´ï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£ã€å½¢çŠ¶ã€ã‚¨ãƒƒã‚¸ç­‰ï¼‰ã§åˆ¤æ–­ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„\"\n",
    "        )\n",
    "    elif rgb_mean_significant:\n",
    "        print(\"\\nğŸ”´ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯RGBå¹³å‡å€¤ã«ç‰¹å¾´ãŒã‚ã‚‹\")\n",
    "        print(\"   â†’ ç‰¹å®šã®è‰²åŸŸã§èª¤åˆ†é¡ãŒèµ·ãã‚„ã™ã„\")\n",
    "    elif rgb_std_significant:\n",
    "        print(\"\\nğŸŸ¡ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯RGBåˆ†æ•£ï¼ˆç”»åƒå†…ã®ã°ã‚‰ã¤ãï¼‰ã«ç‰¹å¾´ãŒã‚ã‚‹\")\n",
    "        print(\"   â†’ å‡ä¸€ãªè‰²ã®ç”»åƒ or è‰²ãŒè¤‡é›‘ãªç”»åƒã§èª¤åˆ†é¡ã—ã‚„ã™ã„å¯èƒ½æ€§\")\n",
    "\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "analyze_rgb_distribution_detail(x_test, y_test, pred_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48527def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_water_vs_vegetation(x_test, y_test, pred_labels, class_names):\n",
    "    \"\"\"\n",
    "    æ°´åŸŸï¼ˆSeaLake, Riverï¼‰ã¨ç·‘ç³»æ¤ç”Ÿï¼ˆPasture, AnnualCropï¼‰ã®è‰²åˆ†å¸ƒã‚’æ¯”è¼ƒ\n",
    "    â†’ æ°´åŸŸã«ç·‘è‰²ãŒå­˜åœ¨ã™ã‚‹ã¨ã€æ¤ç”Ÿã¨èª¤åˆ†é¡ã•ã‚Œã‚„ã™ã„ã“ã¨ã‚’æ¤œè¨¼\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy import stats\n",
    "\n",
    "    # æ¯”è¼ƒã™ã‚‹ã‚¯ãƒ©ã‚¹ãƒšã‚¢ï¼ˆæ°´åŸŸ vs ç·‘ç³»æ¤ç”Ÿï¼‰\n",
    "    pairs_to_compare = [\n",
    "        (\"SeaLake\", \"Pasture\", \"æ¹–æ²¼ã®ç·‘ â†’ ç‰§è‰åœ°ã¨èª¤èª\"),\n",
    "        (\"River\", \"AnnualCrop\", \"æ²³å·ã®ç·‘ â†’ ä½œç‰©ã¨èª¤èª\"),\n",
    "    ]\n",
    "\n",
    "    # ã‚¯ãƒ©ã‚¹åã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "    class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "    # RGBå¹³å‡å€¤\n",
    "    rgb_means = np.mean(x_test, axis=(1, 2)) * 255\n",
    "\n",
    "    # æ­£è§£/èª¤åˆ†é¡ã®ãƒã‚¹ã‚¯\n",
    "    correct_mask = pred_labels == y_test\n",
    "    incorrect_mask = ~correct_mask\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    channel_names = [\"Red\", \"Green\", \"Blue\"]\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ã€æ°´åŸŸã®ç·‘è‰²æˆåˆ†ãŒæ¤ç”Ÿã¨ã®èª¤åˆ†é¡ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ã®æ¤œè¨¼ã€‘\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nä»®èª¬: SeaLakeã‚„Riverã«è—»é¡ç­‰ã®ç·‘è‰²ãŒå­˜åœ¨ã™ã‚‹ã¨ã€\")\n",
    "    print(\"      Pastureï¼ˆç‰§è‰åœ°ï¼‰ã‚„AnnualCropï¼ˆä½œç‰©ï¼‰ã¨èª¤åˆ†é¡ã•ã‚Œã‚„ã™ã„\")\n",
    "\n",
    "    for row, (water_class, veg_class, description) in enumerate(pairs_to_compare):\n",
    "        if water_class not in class_to_idx or veg_class not in class_to_idx:\n",
    "            print(f\"âš ï¸ {water_class} ã¾ãŸã¯ {veg_class} ãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å­˜åœ¨ã—ã¾ã›ã‚“\")\n",
    "            continue\n",
    "\n",
    "        idx_water = class_to_idx[water_class]\n",
    "        idx_veg = class_to_idx[veg_class]\n",
    "\n",
    "        # æ°´åŸŸã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¼ã‚¿\n",
    "        mask_water_all = y_test == idx_water\n",
    "        mask_water_correct = mask_water_all & correct_mask\n",
    "        mask_water_wrong_to_veg = mask_water_all & (pred_labels == idx_veg)\n",
    "\n",
    "        # æ¤ç”Ÿã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¼ã‚¿\n",
    "        mask_veg = y_test == idx_veg\n",
    "\n",
    "        rgb_water_correct = rgb_means[mask_water_correct]  # æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸæ°´åŸŸ\n",
    "        rgb_water_wrong = rgb_means[mask_water_wrong_to_veg]  # æ¤ç”Ÿã¨èª¤åˆ†é¡ã•ã‚ŒãŸæ°´åŸŸ\n",
    "        rgb_veg = rgb_means[mask_veg]  # æ¤ç”Ÿã‚¯ãƒ©ã‚¹å…¨ä½“\n",
    "\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"â–  {description}\")\n",
    "        print(f\"  {water_class}ï¼ˆæ°´åŸŸï¼‰â†’ {veg_class}ï¼ˆæ¤ç”Ÿï¼‰ã¸ã®èª¤åˆ†é¡\")\n",
    "        print(f\"  ã‚µãƒ³ãƒ—ãƒ«æ•°:\")\n",
    "        print(f\"    - {water_class} æ­£è§£: {len(rgb_water_correct)}ä»¶\")\n",
    "        print(f\"    - {water_class} â†’ {veg_class} èª¤åˆ†é¡: {len(rgb_water_wrong)}ä»¶\")\n",
    "        print(f\"    - {veg_class} å…¨ä½“: {len(rgb_veg)}ä»¶\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # --- 1. å„ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ æ¯”è¼ƒ ---\n",
    "        for ch, ch_name in enumerate(channel_names):\n",
    "            ax = axes[row, ch]\n",
    "\n",
    "            # æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸæ°´åŸŸï¼ˆé’è‰²ï¼‰\n",
    "            if len(rgb_water_correct) > 0:\n",
    "                ax.hist(\n",
    "                    rgb_water_correct[:, ch],\n",
    "                    bins=30,\n",
    "                    alpha=0.5,\n",
    "                    label=f\"{water_class} (æ­£è§£)\",\n",
    "                    color=\"#2196F3\",\n",
    "                    density=True,\n",
    "                )\n",
    "\n",
    "            # æ¤ç”Ÿã¨èª¤åˆ†é¡ã•ã‚ŒãŸæ°´åŸŸï¼ˆèµ¤è‰²ã€å¼·èª¿ï¼‰\n",
    "            if len(rgb_water_wrong) > 0:\n",
    "                ax.hist(\n",
    "                    rgb_water_wrong[:, ch],\n",
    "                    bins=20,\n",
    "                    alpha=0.8,\n",
    "                    label=f\"{water_class}â†’{veg_class} (èª¤åˆ†é¡)\",\n",
    "                    color=\"#F44336\",\n",
    "                    density=True,\n",
    "                )\n",
    "\n",
    "            # æ¤ç”Ÿã‚¯ãƒ©ã‚¹ï¼ˆç·‘è‰²ï¼‰\n",
    "            ax.hist(\n",
    "                rgb_veg[:, ch],\n",
    "                bins=30,\n",
    "                alpha=0.5,\n",
    "                label=f\"{veg_class} (æ¤ç”Ÿ)\",\n",
    "                color=\"#4CAF50\",\n",
    "                density=True,\n",
    "            )\n",
    "\n",
    "            # å¹³å‡å€¤ã®ç¸¦ç·š\n",
    "            if len(rgb_water_correct) > 0:\n",
    "                ax.axvline(\n",
    "                    np.mean(rgb_water_correct[:, ch]),\n",
    "                    color=\"#1565C0\",\n",
    "                    linestyle=\"--\",\n",
    "                    linewidth=2,\n",
    "                )\n",
    "            if len(rgb_water_wrong) > 0:\n",
    "                ax.axvline(\n",
    "                    np.mean(rgb_water_wrong[:, ch]),\n",
    "                    color=\"#C62828\",\n",
    "                    linestyle=\"-\",\n",
    "                    linewidth=3,\n",
    "                    label=\"èª¤åˆ†é¡ã®å¹³å‡\",\n",
    "                )\n",
    "            ax.axvline(\n",
    "                np.mean(rgb_veg[:, ch]), color=\"#2E7D32\", linestyle=\"--\", linewidth=2\n",
    "            )\n",
    "\n",
    "            ax.set_xlabel(f\"{ch_name} Value (0-255)\", fontsize=11)\n",
    "            ax.set_ylabel(\"Density\", fontsize=11)\n",
    "            ax.set_title(\n",
    "                f\"{ch_name} Channel\\n({water_class} vs {veg_class})\",\n",
    "                fontsize=12,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "            ax.legend(fontsize=8, loc=\"upper right\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "            # çµ±è¨ˆåˆ†æ\n",
    "            if len(rgb_water_wrong) > 0 and len(rgb_water_correct) > 0:\n",
    "                mean_correct = np.mean(rgb_water_correct[:, ch])\n",
    "                mean_wrong = np.mean(rgb_water_wrong[:, ch])\n",
    "                mean_veg = np.mean(rgb_veg[:, ch])\n",
    "\n",
    "                print(f\"\\n  ã€{ch_name}ãƒãƒ£ãƒ³ãƒãƒ«ã€‘\")\n",
    "                print(f\"    {water_class} æ­£è§£: {mean_correct:.1f}\")\n",
    "                print(f\"    {water_class} èª¤åˆ†é¡: {mean_wrong:.1f}\")\n",
    "                print(f\"    {veg_class}: {mean_veg:.1f}\")\n",
    "\n",
    "                # èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ãŒæ¤ç”Ÿã«è¿‘ã„ã‹ï¼Ÿ\n",
    "                dist_to_correct = abs(mean_wrong - mean_correct)\n",
    "                dist_to_veg = abs(mean_wrong - mean_veg)\n",
    "\n",
    "                if ch == 1:  # Green ãƒãƒ£ãƒ³ãƒãƒ«ãŒç‰¹ã«é‡è¦\n",
    "                    if dist_to_veg < dist_to_correct:\n",
    "                        print(\n",
    "                            f\"    â†’ ğŸ”´ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯{veg_class}ã«è¿‘ã„è‰²ï¼ˆç·‘ãŒå¼·ã„ï¼‰\"\n",
    "                        )\n",
    "                    else:\n",
    "                        print(f\"    â†’ ğŸŸ¢ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯å…ƒã®{water_class}ã«è¿‘ã„è‰²\")\n",
    "\n",
    "        # --- 2. Green vs Blue 2Dãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ°´åŸŸã¨æ¤ç”Ÿã®åˆ†é›¢ã«é‡è¦ï¼‰---\n",
    "        ax = axes[row, 3]\n",
    "\n",
    "        # æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸæ°´åŸŸ\n",
    "        if len(rgb_water_correct) > 0:\n",
    "            ax.scatter(\n",
    "                rgb_water_correct[:, 1],\n",
    "                rgb_water_correct[:, 2],\n",
    "                alpha=0.3,\n",
    "                label=f\"{water_class} (æ­£è§£)\",\n",
    "                c=\"#2196F3\",\n",
    "                s=15,\n",
    "            )\n",
    "\n",
    "        # æ¤ç”Ÿã¨èª¤åˆ†é¡ã•ã‚ŒãŸæ°´åŸŸï¼ˆå¤§ããå¼·èª¿ï¼‰\n",
    "        if len(rgb_water_wrong) > 0:\n",
    "            ax.scatter(\n",
    "                rgb_water_wrong[:, 1],\n",
    "                rgb_water_wrong[:, 2],\n",
    "                alpha=1.0,\n",
    "                label=f\"{water_class}â†’{veg_class} (èª¤åˆ†é¡)\",\n",
    "                c=\"#F44336\",\n",
    "                s=80,\n",
    "                marker=\"X\",\n",
    "                edgecolors=\"black\",\n",
    "                linewidth=1,\n",
    "            )\n",
    "\n",
    "        # æ¤ç”Ÿã‚¯ãƒ©ã‚¹\n",
    "        ax.scatter(\n",
    "            rgb_veg[:, 1],\n",
    "            rgb_veg[:, 2],\n",
    "            alpha=0.3,\n",
    "            label=f\"{veg_class}\",\n",
    "            c=\"#4CAF50\",\n",
    "            s=15,\n",
    "        )\n",
    "\n",
    "        # é‡å¿ƒ\n",
    "        if len(rgb_water_correct) > 0:\n",
    "            center_water = np.mean(rgb_water_correct, axis=0)\n",
    "            ax.scatter(\n",
    "                center_water[1],\n",
    "                center_water[2],\n",
    "                marker=\"â˜…\",\n",
    "                s=300,\n",
    "                c=\"#1565C0\",\n",
    "                edgecolors=\"white\",\n",
    "                linewidth=2,\n",
    "                zorder=10,\n",
    "            )\n",
    "\n",
    "        if len(rgb_water_wrong) > 0:\n",
    "            center_wrong = np.mean(rgb_water_wrong, axis=0)\n",
    "            ax.scatter(\n",
    "                center_wrong[1],\n",
    "                center_wrong[2],\n",
    "                marker=\"â˜…\",\n",
    "                s=300,\n",
    "                c=\"#C62828\",\n",
    "                edgecolors=\"white\",\n",
    "                linewidth=2,\n",
    "                zorder=10,\n",
    "            )\n",
    "\n",
    "        center_veg = np.mean(rgb_veg, axis=0)\n",
    "        ax.scatter(\n",
    "            center_veg[1],\n",
    "            center_veg[2],\n",
    "            marker=\"â˜…\",\n",
    "            s=300,\n",
    "            c=\"#2E7D32\",\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=2,\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(\"Green Mean\", fontsize=11)\n",
    "        ax.set_ylabel(\"Blue Mean\", fontsize=11)\n",
    "        ax.set_title(\n",
    "            f\"Green vs Blue\\nèª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯ç·‘æ–¹å‘ã«ã‚·ãƒ•ãƒˆï¼Ÿ\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.legend(fontsize=9, loc=\"upper left\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # å®šé‡åˆ†æ\n",
    "        if len(rgb_water_wrong) > 0 and len(rgb_water_correct) > 0:\n",
    "            center_water = np.mean(rgb_water_correct, axis=0)\n",
    "            center_wrong = np.mean(rgb_water_wrong, axis=0)\n",
    "            center_veg = np.mean(rgb_veg, axis=0)\n",
    "\n",
    "            dist_wrong_to_water = np.linalg.norm(center_wrong - center_water)\n",
    "            dist_wrong_to_veg = np.linalg.norm(center_wrong - center_veg)\n",
    "\n",
    "            print(f\"\\n  ã€RGBç©ºé–“ã§ã®ä½ç½®é–¢ä¿‚ã€‘\")\n",
    "            print(f\"    èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ« â†’ {water_class}é‡å¿ƒ: {dist_wrong_to_water:.2f}\")\n",
    "            print(f\"    èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ« â†’ {veg_class}é‡å¿ƒ: {dist_wrong_to_veg:.2f}\")\n",
    "\n",
    "            # Greenæˆåˆ†ã®æ¯”è¼ƒ\n",
    "            green_shift = center_wrong[1] - center_water[1]\n",
    "            print(f\"    Greenæˆåˆ†ã®ã‚·ãƒ•ãƒˆ: {green_shift:+.2f}\")\n",
    "\n",
    "            if dist_wrong_to_veg < dist_wrong_to_water:\n",
    "                print(f\"\\n    âœ… çµè«–: èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯RGBç©ºé–“ã§{veg_class}ã«è¿‘ã„\")\n",
    "                print(\n",
    "                    f\"       â†’ ç·‘è‰²æˆåˆ†ãŒå¼·ã„{water_class}ç”»åƒãŒ{veg_class}ã¨èª¤èªã•ã‚ŒãŸ\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"\\n    âŒ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã¯{water_class}ã®é ˜åŸŸå†…ã«ã‚ã‚‹\")\n",
    "                print(f\"       â†’ è‰²ä»¥å¤–ã®è¦å› ï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£ç­‰ï¼‰ãŒèª¤åˆ†é¡ã®åŸå› ã®å¯èƒ½æ€§\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"æ°´åŸŸã®ç·‘è‰²æˆåˆ†ã¨æ¤ç”Ÿã¸ã®èª¤åˆ†é¡ã®é–¢ä¿‚\\nï¼ˆèµ¤X = èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ï¼‰\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"water_vegetation_misclassification.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"\\nå›³ã‚’ä¿å­˜ã—ã¾ã—ãŸ: water_vegetation_misclassification.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # ç·æ‹¬\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ã€ç·æ‹¬ã€‘\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\"\"\n",
    "â–  ä»®èª¬ã®æ¤œè¨¼çµæœ:\n",
    "  æ°´åŸŸï¼ˆSeaLake, Riverï¼‰ã«ç·‘è‰²æˆåˆ†ï¼ˆè—»é¡ã€æ°´è‰ç­‰ï¼‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€\n",
    "  ç·‘ç³»æ¤ç”Ÿï¼ˆPasture, AnnualCropï¼‰ã¨èª¤åˆ†é¡ã•ã‚Œã‚„ã™ã„ã€‚\n",
    "\n",
    "â–  èª¤åˆ†é¡ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ :\n",
    "  1. é€šå¸¸ã®æ°´åŸŸ: é’è‰²ãŒå¼·ãã€Greenå€¤ãŒä½ã„ â†’ æ­£ã—ãæ°´åŸŸã¨åˆ¤å®š\n",
    "  2. ç·‘ãŒã‹ã£ãŸæ°´åŸŸ: è—»é¡ç­‰ã§Greenå€¤ãŒä¸Šæ˜‡ â†’ æ¤ç”Ÿã«ä¼¼ãŸè‰²ã«ãªã‚‹\n",
    "  3. ãƒ¢ãƒ‡ãƒ«ã®åˆ¤æ–­: ã€Œç·‘ãŒå¼·ã„ = æ¤ç‰©ã€ã¨å­¦ç¿’ã—ã¦ã„ã‚‹ãŸã‚èª¤åˆ†é¡\n",
    "\n",
    "â–  å¯¾ç­–æ¡ˆ:\n",
    "  â€¢ ãƒ†ã‚¯ã‚¹ãƒãƒ£ç‰¹å¾´ã®æ´»ç”¨ï¼ˆæ°´é¢ã®åå°„ãƒ‘ã‚¿ãƒ¼ãƒ³ vs æ¤ç”Ÿã®ãƒ†ã‚¯ã‚¹ãƒãƒ£ï¼‰\n",
    "  â€¢ ç©ºé–“çš„ç‰¹å¾´ã®è€ƒæ…®ï¼ˆæ¹–ã®å½¢çŠ¶ vs æ¤ç‰©ã®ä¸å‡ä¸€ãªåˆ†å¸ƒï¼‰\n",
    "  â€¢ NIRï¼ˆè¿‘èµ¤å¤–ç·šï¼‰ãƒãƒ³ãƒ‰ã®è¿½åŠ ï¼ˆæ°´ã¨æ¤ç‰©ã¯åå°„ç‰¹æ€§ãŒç•°ãªã‚‹ï¼‰\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "compare_water_vs_vegetation(x_test, y_test, pred_labels, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
